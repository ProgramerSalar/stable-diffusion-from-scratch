# OpenAI-Style UNet Model for Diffusion and Generative Tasks

This module implements a highly flexible, OpenAI-style UNet architecture with advanced attention mechanisms, including FlashAttention, for use in diffusion models and other generative frameworks. The design is inspired by architectures used in Stable Diffusion and Denoising Diffusion Probabilistic Models (DDPM).

---

## Table of Contents

- [Overview](#overview)
- [Features](#features)
- [File Structure](#file-structure)
- [Installation](#installation)
- [Usage](#usage)
  - [Model Construction](#model-construction)
  - [Forward Pass Example](#forward-pass-example)
- [Architecture Details](#architecture-details)
  - [Attention Mechanisms](#attention-mechanisms)
  - [Residual Blocks](#residual-blocks)
  - [Downsampling/Upsampling](#downsamplingupsampling)
  - [Timestep Embedding](#timestep-embedding)
- [Customization](#customization)
- [Extending the Model](#extending-the-model)


---

## Overview

This package provides a modular UNet implementation with support for:

- Residual connections
- Group normalization
- Multiple attention types (vanilla, linear, FlashAttention)
- Timestep embeddings for diffusion models
- Flexible channel scaling and depth
- Optional spatial transformers for cross-attention

The model is suitable for use in diffusion models, autoencoders, and other image generation tasks.

---

## Features

- **PyTorch Implementation**: Fully implemented using PyTorch, easy to integrate with other PyTorch-based projects.
- **Advanced Attention**: Supports vanilla, linear, and FlashAttention for efficient memory usage and speed.
- **Residual Blocks**: Uses ResNet-style blocks for stable training and deep architectures.
- **Flexible Architecture**: Easily configurable number of channels, resolution, attention resolutions, and more.
- **Timestep Embedding**: Supports timestep embeddings for use in diffusion models.
- **Modular Design**: Components such as attention, normalization, and up/downsampling are modular and reusable.
- **Mixed Precision & GPU Support**: Ready for fast training on modern hardware.

---

## File Structure
```
openai_model/ 
├── model.py # Main UNet model and building blocks 
├── attention.py # Attention modules (vanilla, linear, FlashAttention) 
├── utils.py # Utility functions (conv_nd, normalization, etc.) 
├── Readme.md # This documentation
```


---

## Installation

1. Clone the repository:
    ```bash
    git clone https://github.com/your-repo/stable-diffusion-from-scratch.git
    cd stable-diffusion-from-scratch
    ```

2. Install dependencies:
    ```bash
    pip install torch einops flash_attn
    ```

---

## Usage

### Model Construction

You can construct a UNet model with custom parameters as follows:

```python
from openai_model.model import UNetModel

unet = UNetModel(
    image_size=32,
    in_channels=4,
    out_channels=4,
    model_channels=320,
    attention_resolutions=[4, 2, 1],
    num_res_blocks=2,
    channel_mult=(1, 2, 4, 4),
    num_heads=8,
    use_spatial_transformer=True,
    transformer_depth=1,
    context_dim=768,
    use_checkpoint=True,
    legacy=False,
    use_fp16=True,
    resblock_updown=True,
    use_scale_shift_norm=True,
    num_classes=10,
).half().cuda()

```

Forward pass Example 

```
import torch

batch_size = 4
x = torch.randn(batch_size, 4, 64, 64).half().cuda()
timesteps = torch.tensor([1000] * batch_size).cuda()
context = torch.randn(batch_size, 77, 768).half().cuda()
y = torch.randint(0, 10, (batch_size,)).cuda()

with torch.no_grad():
    output = unet(x, timesteps, context, y)

print(output.shape)
```

## Architecture Details
Attention Mechanisms
The model supports several attention types, implemented in `openai_model/attention.py`:

**Vanilla Attention**: Standard self-attention as in transformers.
**Linear Attention**: Efficient attention mechanism using einsum and rearrange.
**FlashAttention**: Highly efficient attention using the flash_attn library for large-scale models.

You can select the attention type via the `make_attention` function.

Residual Blocks

The `ResBlock` class implements a standard residual block with optional convolutional shortcut and timestep embedding projection. This is used throughout the UNet for stable and deep architectures.

Downsampling/Upsampling

- `Downsample`: Reduces spatial resolution, optionally with a convolution.
- `Upsample`: Increases spatial resolution, optionally with a convolution.

Timestep Embedding

The model supports timestep embeddings for use in diffusion models. The embedding is generated by `timestep_embedding` and projected through two linear layers.


## Customization
You can customize the UNet by changing the following parameters:

- `in_channels`: Number of input channels.
- `out_channels`: Number of output channels.
- `model_channels`: Base channel count.
- `channel_mult`: Tuple of multipliers for each resolution level.
- `num_res_blocks`: Number of residual blocks per resolution.
- `attention_resolutions`: List of resolutions at which to apply attention.
- `dropout`: Dropout rate in residual blocks.
- `use_spatial_transformer`: Enable spatial transformer blocks for cross-attention.
- `context_dim`: Dimension of context for cross-attention.
- `num_heads`: Number of attention heads.
- `use_fp16`: Enable mixed precision.
- `resblock_updown`: Use residual blocks for up/downsampling.


## Extending the Model
- **Custom Attention**: Implement new attention mechanisms in `attention.py` and update `make_attention`.
- **Conditional Inputs**: Pass additional context to the model via the context argument in the forward method.
- **Different Normalization**: Swap out `Normalize` for other normalization layers as needed.
Custom Residual Blocks: Extend `ResBlock` for more complex architectures.