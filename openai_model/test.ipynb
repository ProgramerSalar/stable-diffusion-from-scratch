{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15a4a3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 257])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "\n",
    "\n",
    "tensor1 = torch.randn(1, 3, 256)\n",
    "tensor2 = torch.ones_like(tensor1)\n",
    "tensor3 = torch.randn(1, 3, 1)\n",
    "\n",
    "cat = torch.cat([tensor1, tensor3], dim=-1)\n",
    "print(cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abec7ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default weight dtype: torch.float16\n",
      "output data shape = torch.Size([4, 1024, 320]),  and dtype = torch.float16\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "\n",
    "dim = 320\n",
    "norm1 = nn.LayerNorm(dim)\n",
    "\n",
    "# print(f\"Default weight dtype: {norm1.weight.dtype} and weight: {norm1.weight}\")\n",
    "\n",
    "x = torch.randn(4, 1024, 320).half()\n",
    "output  = norm1(x)\n",
    "print(f\"Default weight dtype: {norm1.weight.dtype}\")\n",
    "\n",
    "print(f\"output data shape = {output.shape},  and dtype = {output.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e7ab63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check the data type : torch.Size([4, 1024, 320]) and dtype: >> torch.float16\n",
      "check then norm weight dtype: torch.float16\n",
      "tensor([[[ 0.4531, -1.0840,  0.9790,  ...,  0.1091,  1.0312,  1.4365],\n",
      "         [-0.9214, -0.5029,  0.5869,  ...,  0.5444, -0.8643, -1.2686],\n",
      "         [-0.5210, -0.6621, -1.2920,  ...,  0.1616,  0.0736, -0.2712],\n",
      "         ...,\n",
      "         [ 1.8242,  0.5103,  1.2871,  ..., -1.3418, -0.5596,  0.0524],\n",
      "         [ 0.4832,  0.0952, -0.6973,  ..., -0.3154, -0.5054, -1.1504],\n",
      "         [-1.7412, -0.5029,  0.4231,  ..., -0.4661, -1.7686, -0.6753]],\n",
      "\n",
      "        [[ 1.4941, -1.1133, -0.6704,  ..., -0.5112,  0.4390,  0.7363],\n",
      "         [-0.3074, -0.4773,  0.7939,  ...,  1.7012,  1.6094,  0.0892],\n",
      "         [-0.8442,  0.5566,  1.8379,  ...,  0.9502, -1.0732, -0.2891],\n",
      "         ...,\n",
      "         [ 0.4026, -0.7671,  1.8018,  ..., -1.2324, -0.0320, -1.1357],\n",
      "         [-0.4087,  0.0795, -0.9907,  ...,  0.4678,  0.9648,  0.4980],\n",
      "         [ 1.4229, -1.6201,  1.1826,  ..., -0.2058, -1.6035, -1.6338]],\n",
      "\n",
      "        [[ 1.2002,  0.4612, -0.8330,  ..., -0.4827,  0.2808,  0.1915],\n",
      "         [-0.6685, -0.0316, -0.1096,  ..., -0.4001, -3.1680, -0.4922],\n",
      "         [-0.0811,  0.6250,  0.0642,  ..., -1.5967,  0.0548,  0.5591],\n",
      "         ...,\n",
      "         [-0.4692, -1.5166, -0.5718,  ...,  0.9209,  1.0029,  0.2306],\n",
      "         [-0.1226,  0.5752,  0.5557,  ..., -0.3037, -1.1582,  0.0938],\n",
      "         [-0.9844,  0.3188, -0.7466,  ...,  1.6787,  1.0859,  0.2776]],\n",
      "\n",
      "        [[ 1.9346,  0.8735,  0.2450,  ...,  0.8599,  0.1153,  0.3538],\n",
      "         [-0.1484, -0.3711,  0.0328,  ...,  1.7832, -0.3792, -0.0127],\n",
      "         [ 1.7559, -1.0615, -0.3201,  ..., -1.6182,  1.3418,  0.3940],\n",
      "         ...,\n",
      "         [ 0.4697, -0.5273, -0.0233,  ..., -1.0234, -0.5732, -1.2100],\n",
      "         [-0.7485, -0.2705, -0.4268,  ...,  0.8638, -0.7036, -0.1576],\n",
      "         [ 0.6816,  1.8213, -1.7451,  ...,  0.7246,  0.2228,  2.0996]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "\n",
    "class Testing(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim, \n",
    "                 ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        \n",
    "\n",
    "    def forward(self, x, context=None):\n",
    "      \n",
    "        print(f\"check the data type : {x.shape} and dtype: >> {x.dtype}\")   # torch.Size([4, 1024, 320]) and dtype: >> torch.float16\n",
    "\n",
    "        print(f\"check then norm weight dtype: {self.norm1.weight.dtype}\")   # torch.float32\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "\n",
    "        return x \n",
    "    \n",
    "\n",
    "output = Testing(dim=320).cuda().half()\n",
    "    \n",
    "x = torch.randn(4, 1024, 320).half().cuda()\n",
    "context = torch.randn(4, 77, 768).half().cuda()\n",
    "\n",
    "output = output(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a94bef",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685e5132",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda121",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
