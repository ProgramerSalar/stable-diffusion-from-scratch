# 1. Weight size of stable-diffusion

```python
model name:  encoder.conv_in.weight and shape : torch.Size([128, 3, 3, 3])     ->  (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
model name:  encoder.conv_in.bias and shape : torch.Size([128])

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< ECNDOER >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
========================================================================== DOWN BLock ==========================================================================================
<------------------------Resnet Block (Block-0) [Module-0]---------------------------------->
model name:  encoder.down.0.block.0.norm1.weight and shape : torch.Size([128])  -> (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)
model name:  encoder.down.0.block.0.norm1.bias and shape : torch.Size([128])
model name:  encoder.down.0.block.0.conv1.weight and shape : torch.Size([128, 128, 3, 3])   ->  (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
model name:  encoder.down.0.block.0.conv1.bias and shape : torch.Size([128])
model name:  encoder.down.0.block.0.norm2.weight and shape : torch.Size([128])  ->   (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)
model name:  encoder.down.0.block.0.norm2.bias and shape : torch.Size([128])
model name:  encoder.down.0.block.0.conv2.weight and shape : torch.Size([128, 128, 3, 3])   ->  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
model name:  encoder.down.0.block.0.conv2.bias and shape : torch.Size([128])
                                    (Block-1)
model name:  encoder.down.0.block.1.norm1.weight and shape : torch.Size([128])
model name:  encoder.down.0.block.1.norm1.bias and shape : torch.Size([128])
model name:  encoder.down.0.block.1.conv1.weight and shape : torch.Size([128, 128, 3, 3])
model name:  encoder.down.0.block.1.conv1.bias and shape : torch.Size([128])
model name:  encoder.down.0.block.1.norm2.weight and shape : torch.Size([128])
model name:  encoder.down.0.block.1.norm2.bias and shape : torch.Size([128])
model name:  encoder.down.0.block.1.conv2.weight and shape : torch.Size([128, 128, 3, 3])
model name:  encoder.down.0.block.1.conv2.bias and shape : torch.Size([128])

<------------------------- [Downsample] ---------------------------------------->
model name:  encoder.down.0.downsample.conv.weight and shape : torch.Size([128, 128, 3, 3])
model name:  encoder.down.0.downsample.conv.bias and shape : torch.Size([128])

<--------------------------Resnet Block (Block-0) [Module-1]------------------------------->
model name:  encoder.down.1.block.0.norm1.weight and shape : torch.Size([128]) -> (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)
model name:  encoder.down.1.block.0.norm1.bias and shape : torch.Size([128])
model name:  encoder.down.1.block.0.conv1.weight and shape : torch.Size([256, 128, 3, 3])   ->  (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
model name:  encoder.down.1.block.0.conv1.bias and shape : torch.Size([256])
model name:  encoder.down.1.block.0.norm2.weight and shape : torch.Size([256])  ->  (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
model name:  encoder.down.1.block.0.norm2.bias and shape : torch.Size([256])
model name:  encoder.down.1.block.0.conv2.weight and shape : torch.Size([256, 256, 3, 3])   -> (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
model name:  encoder.down.1.block.0.conv2.bias and shape : torch.Size([256])
model name:  encoder.down.1.block.0.nin_shortcut.weight and shape : torch.Size([256, 128, 1, 1])    ->  (nin_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
model name:  encoder.down.1.block.0.nin_shortcut.bias and shape : torch.Size([256])
                                        (Block-1)
model name:  encoder.down.1.block.1.norm1.weight and shape : torch.Size([256])  ->  (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
model name:  encoder.down.1.block.1.norm1.bias and shape : torch.Size([256])
model name:  encoder.down.1.block.1.conv1.weight and shape : torch.Size([256, 256, 3, 3])   -> (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
model name:  encoder.down.1.block.1.conv1.bias and shape : torch.Size([256])
model name:  encoder.down.1.block.1.norm2.weight and shape : torch.Size([256])  ->  (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
model name:  encoder.down.1.block.1.norm2.bias and shape : torch.Size([256])
model name:  encoder.down.1.block.1.conv2.weight and shape : torch.Size([256, 256, 3, 3])   -> (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
model name:  encoder.down.1.block.1.conv2.bias and shape : torch.Size([256])

<------------------------- [Downsample] ---------------------------------------->
model name:  encoder.down.1.downsample.conv.weight and shape : torch.Size([256, 256, 3, 3])
model name:  encoder.down.1.downsample.conv.bias and shape : torch.Size([256])


<------------------------ Resnet Block (Block-0) [Module-2] --------------------------------------------->
model name:  encoder.down.2.block.0.norm1.weight and shape : torch.Size([256])  -> (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
model name:  encoder.down.2.block.0.norm1.bias and shape : torch.Size([256])
model name:  encoder.down.2.block.0.conv1.weight and shape : torch.Size([512, 256, 3, 3])   ->  (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
model name:  encoder.down.2.block.0.conv1.bias and shape : torch.Size([512])
model name:  encoder.down.2.block.0.norm2.weight and shape : torch.Size([512])  -> (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
model name:  encoder.down.2.block.0.norm2.bias and shape : torch.Size([512])
model name:  encoder.down.2.block.0.conv2.weight and shape : torch.Size([512, 512, 3, 3]) -> (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
model name:  encoder.down.2.block.0.conv2.bias and shape : torch.Size([512])
model name:  encoder.down.2.block.0.nin_shortcut.weight and shape : torch.Size([512, 256, 1, 1])    -> (nin_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
model name:  encoder.down.2.block.0.nin_shortcut.bias and shape : torch.Size([512])
                                         (Block-1)
model name:  encoder.down.2.block.1.norm1.weight and shape : torch.Size([512])
model name:  encoder.down.2.block.1.norm1.bias and shape : torch.Size([512])
model name:  encoder.down.2.block.1.conv1.weight and shape : torch.Size([512, 512, 3, 3])
model name:  encoder.down.2.block.1.conv1.bias and shape : torch.Size([512])
model name:  encoder.down.2.block.1.norm2.weight and shape : torch.Size([512])
model name:  encoder.down.2.block.1.norm2.bias and shape : torch.Size([512])
model name:  encoder.down.2.block.1.conv2.weight and shape : torch.Size([512, 512, 3, 3])
model name:  encoder.down.2.block.1.conv2.bias and shape : torch.Size([512])

=============================================================== MIDDLE BLock ===========================================================================
<------------------------------ Mid (Resnet Block-1) ------------------------------------------------------->
model name:  encoder.mid.block_1.norm1.weight and shape : torch.Size([512])
model name:  encoder.mid.block_1.norm1.bias and shape : torch.Size([512])
model name:  encoder.mid.block_1.conv1.weight and shape : torch.Size([512, 512, 3, 3])
model name:  encoder.mid.block_1.conv1.bias and shape : torch.Size([512])
model name:  encoder.mid.block_1.norm2.weight and shape : torch.Size([512])
model name:  encoder.mid.block_1.norm2.bias and shape : torch.Size([512])
model name:  encoder.mid.block_1.conv2.weight and shape : torch.Size([512, 512, 3, 3])
model name:  encoder.mid.block_1.conv2.bias and shape : torch.Size([512])

<--------------------------------Mid (Attention) ----------------------------------------------------->
model name:  encoder.mid.attn_1.norm.weight and shape : torch.Size([512])   -> (norm): GroupNorm(32, 512, eps=1e-05, affine=True)
model name:  encoder.mid.attn_1.norm.bias and shape : torch.Size([512])
model name:  encoder.mid.attn_1.q.weight and shape : torch.Size([512, 512, 1, 1])   -> (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
model name:  encoder.mid.attn_1.q.bias and shape : torch.Size([512])
model name:  encoder.mid.attn_1.k.weight and shape : torch.Size([512, 512, 1, 1])   ->  (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
model name:  encoder.mid.attn_1.k.bias and shape : torch.Size([512])
model name:  encoder.mid.attn_1.v.weight and shape : torch.Size([512, 512, 1, 1])   ->   (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
model name:  encoder.mid.attn_1.v.bias and shape : torch.Size([512])
model name:  encoder.mid.attn_1.proj_out.weight and shape : torch.Size([512, 512, 1, 1])    ->  (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
model name:  encoder.mid.attn_1.proj_out.bias and shape : torch.Size([512])

<------------------------------ Mid (Resnet Block-2) ------------------------------------------------------->
model name:  encoder.mid.block_2.norm1.weight and shape : torch.Size([512])
model name:  encoder.mid.block_2.norm1.bias and shape : torch.Size([512])
model name:  encoder.mid.block_2.conv1.weight and shape : torch.Size([512, 512, 3, 3])
model name:  encoder.mid.block_2.conv1.bias and shape : torch.Size([512])
model name:  encoder.mid.block_2.norm2.weight and shape : torch.Size([512])
model name:  encoder.mid.block_2.norm2.bias and shape : torch.Size([512])
model name:  encoder.mid.block_2.conv2.weight and shape : torch.Size([512, 512, 3, 3])
model name:  encoder.mid.block_2.conv2.bias and shape : torch.Size([512])


model name:  encoder.norm_out.weight and shape : torch.Size([512]) ->  (norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)
model name:  encoder.norm_out.bias and shape : torch.Size([512])
model name:  encoder.conv_out.weight and shape : torch.Size([6, 512, 3, 3]) ->  (conv_out): Conv2d(512, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
model name:  encoder.conv_out.bias and shape : torch.Size([6])

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< DECODER >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
========================================================== Mid Block =================================================================================================================
<---------------------------------------------------------------------------- Resnet Block (Block-1) [Mid] -------------------------------------------------------------------->
model name:  decoder.conv_in.weight and shape : torch.Size([512, 3, 3, 3])  ->  (conv_in): Conv2d(3, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
model name:  decoder.conv_in.bias and shape : torch.Size([512])
model name:  decoder.mid.block_1.norm1.weight and shape : torch.Size([512]) ->   (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
model name:  decoder.mid.block_1.norm1.bias and shape : torch.Size([512])
model name:  decoder.mid.block_1.conv1.weight and shape : torch.Size([512, 512, 3, 3])  ->  (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
model name:  decoder.mid.block_1.conv1.bias and shape : torch.Size([512])
model name:  decoder.mid.block_1.norm2.weight and shape : torch.Size([512]) ->  (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
model name:  decoder.mid.block_1.norm2.bias and shape : torch.Size([512])
model name:  decoder.mid.block_1.conv2.weight and shape : torch.Size([512, 512, 3, 3])  ->  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
model name:  decoder.mid.block_1.conv2.bias and shape : torch.Size([512])

<-------------------------------------- Attention -------------------------------------------------------------------------------------------------->
model name:  decoder.mid.attn_1.norm.weight and shape : torch.Size([512])
model name:  decoder.mid.attn_1.norm.bias and shape : torch.Size([512])
model name:  decoder.mid.attn_1.q.weight and shape : torch.Size([512, 512, 1, 1])
model name:  decoder.mid.attn_1.q.bias and shape : torch.Size([512])
model name:  decoder.mid.attn_1.k.weight and shape : torch.Size([512, 512, 1, 1])
model name:  decoder.mid.attn_1.k.bias and shape : torch.Size([512])
model name:  decoder.mid.attn_1.v.weight and shape : torch.Size([512, 512, 1, 1])
model name:  decoder.mid.attn_1.v.bias and shape : torch.Size([512])
model name:  decoder.mid.attn_1.proj_out.weight and shape : torch.Size([512, 512, 1, 1])
model name:  decoder.mid.attn_1.proj_out.bias and shape : torch.Size([512])

<---------------------------------------------------- Resnet Block (Block-2) -------------------------------------------------------------------------------->
model name:  decoder.mid.block_2.norm1.weight and shape : torch.Size([512])
model name:  decoder.mid.block_2.norm1.bias and shape : torch.Size([512])
model name:  decoder.mid.block_2.conv1.weight and shape : torch.Size([512, 512, 3, 3])
model name:  decoder.mid.block_2.conv1.bias and shape : torch.Size([512])
model name:  decoder.mid.block_2.norm2.weight and shape : torch.Size([512])
model name:  decoder.mid.block_2.norm2.bias and shape : torch.Size([512])
model name:  decoder.mid.block_2.conv2.weight and shape : torch.Size([512, 512, 3, 3])
model name:  decoder.mid.block_2.conv2.bias and shape : torch.Size([512])

============================================================================ UP Block ===================================================================
<---------------------------------------------------------- Resnet Block (Block-0) [Module-0] ---------------------------------------------------------------->
model name:  decoder.up.0.block.0.norm1.weight and shape : torch.Size([256])
model name:  decoder.up.0.block.0.norm1.bias and shape : torch.Size([256])
model name:  decoder.up.0.block.0.conv1.weight and shape : torch.Size([128, 256, 3, 3])
model name:  decoder.up.0.block.0.conv1.bias and shape : torch.Size([128])
model name:  decoder.up.0.block.0.norm2.weight and shape : torch.Size([128])
model name:  decoder.up.0.block.0.norm2.bias and shape : torch.Size([128])
model name:  decoder.up.0.block.0.conv2.weight and shape : torch.Size([128, 128, 3, 3])
model name:  decoder.up.0.block.0.conv2.bias and shape : torch.Size([128])
model name:  decoder.up.0.block.0.nin_shortcut.weight and shape : torch.Size([128, 256, 1, 1])
model name:  decoder.up.0.block.0.nin_shortcut.bias and shape : torch.Size([128])

<---------------------------------------------------------- Resnet Block (Block-1) ---------------------------------------------------------------->
model name:  decoder.up.0.block.1.norm1.weight and shape : torch.Size([128])
model name:  decoder.up.0.block.1.norm1.bias and shape : torch.Size([128])
model name:  decoder.up.0.block.1.conv1.weight and shape : torch.Size([128, 128, 3, 3])
model name:  decoder.up.0.block.1.conv1.bias and shape : torch.Size([128])
model name:  decoder.up.0.block.1.norm2.weight and shape : torch.Size([128])
model name:  decoder.up.0.block.1.norm2.bias and shape : torch.Size([128])
model name:  decoder.up.0.block.1.conv2.weight and shape : torch.Size([128, 128, 3, 3])
model name:  decoder.up.0.block.1.conv2.bias and shape : torch.Size([128])
<---------------------------------------------------------- Resnet Block (Block-2) ---------------------------------------------------------------->
model name:  decoder.up.0.block.2.norm1.weight and shape : torch.Size([128])
model name:  decoder.up.0.block.2.norm1.bias and shape : torch.Size([128])
model name:  decoder.up.0.block.2.conv1.weight and shape : torch.Size([128, 128, 3, 3])
model name:  decoder.up.0.block.2.conv1.bias and shape : torch.Size([128])
model name:  decoder.up.0.block.2.norm2.weight and shape : torch.Size([128])
model name:  decoder.up.0.block.2.norm2.bias and shape : torch.Size([128])
model name:  decoder.up.0.block.2.conv2.weight and shape : torch.Size([128, 128, 3, 3])
model name:  decoder.up.0.block.2.conv2.bias and shape : torch.Size([128])

<---------------------------------------------------------- Resnet Block (Block-0) [Module-1] ---------------------------------------------------------------->
model name:  decoder.up.1.block.0.norm1.weight and shape : torch.Size([512])
model name:  decoder.up.1.block.0.norm1.bias and shape : torch.Size([512])
model name:  decoder.up.1.block.0.conv1.weight and shape : torch.Size([256, 512, 3, 3])
model name:  decoder.up.1.block.0.conv1.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.0.norm2.weight and shape : torch.Size([256])
model name:  decoder.up.1.block.0.norm2.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.0.conv2.weight and shape : torch.Size([256, 256, 3, 3])
model name:  decoder.up.1.block.0.conv2.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.0.nin_shortcut.weight and shape : torch.Size([256, 512, 1, 1])
model name:  decoder.up.1.block.0.nin_shortcut.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.1.norm1.weight and shape : torch.Size([256])
model name:  decoder.up.1.block.1.norm1.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.1.conv1.weight and shape : torch.Size([256, 256, 3, 3])
model name:  decoder.up.1.block.1.conv1.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.1.norm2.weight and shape : torch.Size([256])
model name:  decoder.up.1.block.1.norm2.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.1.conv2.weight and shape : torch.Size([256, 256, 3, 3])
model name:  decoder.up.1.block.1.conv2.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.2.norm1.weight and shape : torch.Size([256])
model name:  decoder.up.1.block.2.norm1.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.2.conv1.weight and shape : torch.Size([256, 256, 3, 3])
model name:  decoder.up.1.block.2.conv1.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.2.norm2.weight and shape : torch.Size([256])
model name:  decoder.up.1.block.2.norm2.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.2.conv2.weight and shape : torch.Size([256, 256, 3, 3])
model name:  decoder.up.1.block.2.conv2.bias and shape : torch.Size([256])

<-------------------------------------------------------------------------- Upsample Block ---------------------------------------------------------------------->
model name:  decoder.up.1.upsample.conv.weight and shape : torch.Size([256, 256, 3, 3])
model name:  decoder.up.1.upsample.conv.bias and shape : torch.Size([256])


<--------------------------------------------------- 3 x Resnet Block [Module-2] -------------------------------------------------------------------->
model name:  decoder.up.2.block.0.norm1.weight and shape : torch.Size([512])
model name:  decoder.up.2.block.0.norm1.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.0.conv1.weight and shape : torch.Size([512, 512, 3, 3])
model name:  decoder.up.2.block.0.conv1.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.0.norm2.weight and shape : torch.Size([512])
model name:  decoder.up.2.block.0.norm2.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.0.conv2.weight and shape : torch.Size([512, 512, 3, 3])
model name:  decoder.up.2.block.0.conv2.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.1.norm1.weight and shape : torch.Size([512])
model name:  decoder.up.2.block.1.norm1.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.1.conv1.weight and shape : torch.Size([512, 512, 3, 3])
model name:  decoder.up.2.block.1.conv1.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.1.norm2.weight and shape : torch.Size([512])
model name:  decoder.up.2.block.1.norm2.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.1.conv2.weight and shape : torch.Size([512, 512, 3, 3])
model name:  decoder.up.2.block.1.conv2.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.2.norm1.weight and shape : torch.Size([512])
model name:  decoder.up.2.block.2.norm1.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.2.conv1.weight and shape : torch.Size([512, 512, 3, 3])
model name:  decoder.up.2.block.2.conv1.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.2.norm2.weight and shape : torch.Size([512])
model name:  decoder.up.2.block.2.norm2.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.2.conv2.weight and shape : torch.Size([512, 512, 3, 3])
model name:  decoder.up.2.block.2.conv2.bias and shape : torch.Size([512])


<-------------------------------------------------------------------------- Upsample Block ---------------------------------------------------------------------->
model name:  decoder.up.2.upsample.conv.weight and shape : torch.Size([512, 512, 3, 3])
model name:  decoder.up.2.upsample.conv.bias and shape : torch.Size([512])


model name:  decoder.norm_out.weight and shape : torch.Size([128])  -> (norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)
model name:  decoder.norm_out.bias and shape : torch.Size([128])
model name:  decoder.conv_out.weight and shape : torch.Size([3, 128, 3, 3]) -> (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
model name:  decoder.conv_out.bias and shape : torch.Size([3])

model name:  loss.logvar and shape : torch.Size([])
model name:  loss.perceptual_loss.scaling_layer.shift and shape : torch.Size([1, 3, 1, 1])
model name:  loss.perceptual_loss.scaling_layer.scale and shape : torch.Size([1, 3, 1, 1])
model name:  loss.perceptual_loss.net.slice1.0.weight and shape : torch.Size([64, 3, 3, 3])
model name:  loss.perceptual_loss.net.slice1.0.bias and shape : torch.Size([64])
model name:  loss.perceptual_loss.net.slice1.2.weight and shape : torch.Size([64, 64, 3, 3])
model name:  loss.perceptual_loss.net.slice1.2.bias and shape : torch.Size([64])
model name:  loss.perceptual_loss.net.slice2.5.weight and shape : torch.Size([128, 64, 3, 3])
model name:  loss.perceptual_loss.net.slice2.5.bias and shape : torch.Size([128])
model name:  loss.perceptual_loss.net.slice2.7.weight and shape : torch.Size([128, 128, 3, 3])
model name:  loss.perceptual_loss.net.slice2.7.bias and shape : torch.Size([128])
model name:  loss.perceptual_loss.net.slice3.10.weight and shape : torch.Size([256, 128, 3, 3])
model name:  loss.perceptual_loss.net.slice3.10.bias and shape : torch.Size([256])
model name:  loss.perceptual_loss.net.slice3.12.weight and shape : torch.Size([256, 256, 3, 3])
model name:  loss.perceptual_loss.net.slice3.12.bias and shape : torch.Size([256])
model name:  loss.perceptual_loss.net.slice3.14.weight and shape : torch.Size([256, 256, 3, 3])
model name:  loss.perceptual_loss.net.slice3.14.bias and shape : torch.Size([256])
model name:  loss.perceptual_loss.net.slice4.17.weight and shape : torch.Size([512, 256, 3, 3])
model name:  loss.perceptual_loss.net.slice4.17.bias and shape : torch.Size([512])
model name:  loss.perceptual_loss.net.slice4.19.weight and shape : torch.Size([512, 512, 3, 3])
model name:  loss.perceptual_loss.net.slice4.19.bias and shape : torch.Size([512])
model name:  loss.perceptual_loss.net.slice4.21.weight and shape : torch.Size([512, 512, 3, 3])
model name:  loss.perceptual_loss.net.slice4.21.bias and shape : torch.Size([512])
model name:  loss.perceptual_loss.net.slice5.24.weight and shape : torch.Size([512, 512, 3, 3])
model name:  loss.perceptual_loss.net.slice5.24.bias and shape : torch.Size([512])
model name:  loss.perceptual_loss.net.slice5.26.weight and shape : torch.Size([512, 512, 3, 3])
model name:  loss.perceptual_loss.net.slice5.26.bias and shape : torch.Size([512])
model name:  loss.perceptual_loss.net.slice5.28.weight and shape : torch.Size([512, 512, 3, 3])
model name:  loss.perceptual_loss.net.slice5.28.bias and shape : torch.Size([512])
model name:  loss.perceptual_loss.lin0.model.1.weight and shape : torch.Size([1, 64, 1, 1])
model name:  loss.perceptual_loss.lin1.model.1.weight and shape : torch.Size([1, 128, 1, 1])
model name:  loss.perceptual_loss.lin2.model.1.weight and shape : torch.Size([1, 256, 1, 1])
model name:  loss.perceptual_loss.lin3.model.1.weight and shape : torch.Size([1, 512, 1, 1])
model name:  loss.perceptual_loss.lin4.model.1.weight and shape : torch.Size([1, 512, 1, 1])
model name:  loss.discriminator.main.0.weight and shape : torch.Size([64, 3, 4, 4])
model name:  loss.discriminator.main.0.bias and shape : torch.Size([64])
model name:  loss.discriminator.main.2.weight and shape : torch.Size([128, 64, 4, 4])
model name:  loss.discriminator.main.3.weight and shape : torch.Size([128])
model name:  loss.discriminator.main.3.bias and shape : torch.Size([128])
model name:  loss.discriminator.main.3.running_mean and shape : torch.Size([128])
model name:  loss.discriminator.main.3.running_var and shape : torch.Size([128])
model name:  loss.discriminator.main.3.num_batches_tracked and shape : torch.Size([])
model name:  loss.discriminator.main.5.weight and shape : torch.Size([256, 128, 4, 4])
model name:  loss.discriminator.main.6.weight and shape : torch.Size([256])
model name:  loss.discriminator.main.6.bias and shape : torch.Size([256])
model name:  loss.discriminator.main.6.running_mean and shape : torch.Size([256])
model name:  loss.discriminator.main.6.running_var and shape : torch.Size([256])
model name:  loss.discriminator.main.6.num_batches_tracked and shape : torch.Size([])
model name:  loss.discriminator.main.8.weight and shape : torch.Size([512, 256, 4, 4])
model name:  loss.discriminator.main.9.weight and shape : torch.Size([512])
model name:  loss.discriminator.main.9.bias and shape : torch.Size([512])
model name:  loss.discriminator.main.9.running_mean and shape : torch.Size([512])
model name:  loss.discriminator.main.9.running_var and shape : torch.Size([512])
model name:  loss.discriminator.main.9.num_batches_tracked and shape : torch.Size([])
model name:  loss.discriminator.main.11.weight and shape : torch.Size([1, 512, 4, 4])
model name:  loss.discriminator.main.11.bias and shape : torch.Size([1])
model name:  quant_conv.weight and shape : torch.Size([6, 6, 1, 1])
model name:  quant_conv.bias and shape : torch.Size([6])
model name:  post_quant_conv.weight and shape : torch.Size([3, 3, 1, 1])
model name:  post_quant_conv.bias and shape : torch.Size([3])

```

Archtecture: 
```
Model:  AutoEncoderKL(
  (encoder): Encoder(
    (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (down): ModuleList(
      (0): Module(
        (block): ModuleList(
          (0-1): 2 x ResnetBlock(
            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)
            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        (attn): ModuleList()
        (downsample): Downsample(
          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))
        )
      )
      (1): Module(
        (block): ModuleList(
          (0): ResnetBlock(
            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)
            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (nin_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
          )
          (1): ResnetBlock(
            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        (attn): ModuleList()
        (downsample): Downsample(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        )
      )
      (2): Module(
        (block): ModuleList(
          (0): ResnetBlock(
            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (nin_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (1): ResnetBlock(
            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        (attn): ModuleList()
      )
    )
    (mid): Module(
      (block_1): ResnetBlock(
        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (attn_1): FlashAttentionBlock(
        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)
        (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
        (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
        (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
        (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (block_2): ResnetBlock(
        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
    (norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)
    (conv_out): Conv2d(512, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (decoder): Decoder(
    (conv_in): Conv2d(3, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (mid): Module(
      (block_1): ResnetBlock(
        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (attn_1): FlashAttentionBlock(
        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)
        (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
        (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
        (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
        (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (block_2): ResnetBlock(
        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
    (up): ModuleList(
      (0): Module(
        (block): ModuleList(
          (0): ResnetBlock(
            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
            (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (nin_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          )
          (1-2): 2 x ResnetBlock(
            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)
            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        (attn): ModuleList()
      )
      (1): Module(
        (block): ModuleList(
          (0): ResnetBlock(
            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
            (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (nin_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          )
          (1-2): 2 x ResnetBlock(
            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        (attn): ModuleList()
        (upsample): Upsample(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (2): Module(
        (block): ModuleList(
          (0-2): 3 x ResnetBlock(
            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        (attn): ModuleList()
        (upsample): Upsample(
          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
    )
    (norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)
    (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  ##################################### I'm in here to understanding ###################################
  (quant_conv): Conv2d(6, 6, kernel_size=(1, 1), stride=(1, 1))
  (post_quant_conv): Conv2d(3, 3, kernel_size=(1, 1), stride=(1, 1))
  (loss): LPIPSWithDiscriminator(
    (perceptual_loss): LPIPS(
      (scaling_layer): ScalingLayer()
      (net): Vgg16(
        (slice1): Sequential(
          (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (3): ReLU(inplace=True)
        )
        (slice2): Sequential(
          (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
          (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (6): ReLU(inplace=True)
          (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (8): ReLU(inplace=True)
        )
        (slice3): Sequential(
          (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
          (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (11): ReLU(inplace=True)
          (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (13): ReLU(inplace=True)
          (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (15): ReLU(inplace=True)
        )
        (slice4): Sequential(
          (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
          (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (18): ReLU(inplace=True)
          (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (20): ReLU(inplace=True)
          (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (22): ReLU(inplace=True)
        )
        (slice5): Sequential(
          (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
          (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (25): ReLU(inplace=True)
          (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (27): ReLU(inplace=True)
          (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (29): ReLU(inplace=True)
        )
      )
      (lin0): NetLinLayer(
        (model): Sequential(
          (0): Dropout(p=0.5, inplace=False)
          (1): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (lin1): NetLinLayer(
        (model): Sequential(
          (0): Dropout(p=0.5, inplace=False)
          (1): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (lin2): NetLinLayer(
        (model): Sequential(
          (0): Dropout(p=0.5, inplace=False)
          (1): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (lin3): NetLinLayer(
        (model): Sequential(
          (0): Dropout(p=0.5, inplace=False)
          (1): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (lin4): NetLinLayer(
        (model): Sequential(
          (0): Dropout(p=0.5, inplace=False)
          (1): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
    )
    (discriminator): NLayerDiscriminator(
      (main): Sequential(
        (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.2, inplace=True)
        (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (4): LeakyReLU(negative_slope=0.2, inplace=True)
        (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
        (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (7): LeakyReLU(negative_slope=0.2, inplace=True)
        (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1), bias=False)
        (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (10): LeakyReLU(negative_slope=0.2, inplace=True)
        (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))
      )
    )
  )
)

```

# 2. This shape of weight to me epoch==10
```

model name:  encoder.conv_in.weight and shape : torch.Size([128, 3, 3, 3])
model name:  encoder.conv_in.bias and shape : torch.Size([128])
model name:  encoder.down.0.block.0.norm1.weight and shape : torch.Size([128])
model name:  encoder.down.0.block.0.norm1.bias and shape : torch.Size([128])
model name:  encoder.down.0.block.0.conv1.weight and shape : torch.Size([128, 128, 3, 3])
model name:  encoder.down.0.block.0.conv1.bias and shape : torch.Size([128])
model name:  encoder.down.0.block.0.norm2.weight and shape : torch.Size([128])
model name:  encoder.down.0.block.0.norm2.bias and shape : torch.Size([128])
model name:  encoder.down.0.block.0.conv2.weight and shape : torch.Size([128, 128, 3, 3])
model name:  encoder.down.0.block.0.conv2.bias and shape : torch.Size([128])
model name:  encoder.down.0.block.1.norm1.weight and shape : torch.Size([128])
model name:  encoder.down.0.block.1.norm1.bias and shape : torch.Size([128])
model name:  encoder.down.0.block.1.conv1.weight and shape : torch.Size([128, 128, 3, 3])
model name:  encoder.down.0.block.1.conv1.bias and shape : torch.Size([128])
model name:  encoder.down.0.block.1.norm2.weight and shape : torch.Size([128])
model name:  encoder.down.0.block.1.norm2.bias and shape : torch.Size([128])
model name:  encoder.down.0.block.1.conv2.weight and shape : torch.Size([128, 128, 3, 3])
model name:  encoder.down.0.block.1.conv2.bias and shape : torch.Size([128])
model name:  encoder.down.0.downsample.conv.weight and shape : torch.Size([128, 128, 3, 3])
model name:  encoder.down.0.downsample.conv.bias and shape : torch.Size([128])
model name:  encoder.down.1.block.0.norm1.weight and shape : torch.Size([128])
model name:  encoder.down.1.block.0.norm1.bias and shape : torch.Size([128])
model name:  encoder.down.1.block.0.conv1.weight and shape : torch.Size([256, 128, 3, 3])
model name:  encoder.down.1.block.0.conv1.bias and shape : torch.Size([256])
model name:  encoder.down.1.block.0.norm2.weight and shape : torch.Size([256])
model name:  encoder.down.1.block.0.norm2.bias and shape : torch.Size([256])
model name:  encoder.down.1.block.0.conv2.weight and shape : torch.Size([256, 256, 3, 3])
model name:  encoder.down.1.block.0.conv2.bias and shape : torch.Size([256])
model name:  encoder.down.1.block.0.nin_shortcut.weight and shape : torch.Size([256, 128, 1, 1])
model name:  encoder.down.1.block.0.nin_shortcut.bias and shape : torch.Size([256])
model name:  encoder.down.1.block.1.norm1.weight and shape : torch.Size([256])
model name:  encoder.down.1.block.1.norm1.bias and shape : torch.Size([256])
model name:  encoder.down.1.block.1.conv1.weight and shape : torch.Size([256, 256, 3, 3])
model name:  encoder.down.1.block.1.conv1.bias and shape : torch.Size([256])
model name:  encoder.down.1.block.1.norm2.weight and shape : torch.Size([256])
model name:  encoder.down.1.block.1.norm2.bias and shape : torch.Size([256])
model name:  encoder.down.1.block.1.conv2.weight and shape : torch.Size([256, 256, 3, 3])
model name:  encoder.down.1.block.1.conv2.bias and shape : torch.Size([256])
model name:  encoder.down.1.downsample.conv.weight and shape : torch.Size([256, 256, 3, 3])
model name:  encoder.down.1.downsample.conv.bias and shape : torch.Size([256])
model name:  encoder.down.2.block.0.norm1.weight and shape : torch.Size([256])
model name:  encoder.down.2.block.0.norm1.bias and shape : torch.Size([256])
model name:  encoder.down.2.block.0.conv1.weight and shape : torch.Size([512, 256, 3, 3])
model name:  encoder.down.2.block.0.conv1.bias and shape : torch.Size([512])
model name:  encoder.down.2.block.0.norm2.weight and shape : torch.Size([512])
model name:  encoder.down.2.block.0.norm2.bias and shape : torch.Size([512])
model name:  encoder.down.2.block.0.conv2.weight and shape : torch.Size([512, 512, 3, 3])
model name:  encoder.down.2.block.0.conv2.bias and shape : torch.Size([512])
model name:  encoder.down.2.block.0.nin_shortcut.weight and shape : torch.Size([512, 256, 1, 1])
model name:  encoder.down.2.block.0.nin_shortcut.bias and shape : torch.Size([512])
model name:  encoder.down.2.block.1.norm1.weight and shape : torch.Size([512])
model name:  encoder.down.2.block.1.norm1.bias and shape : torch.Size([512])
model name:  encoder.down.2.block.1.conv1.weight and shape : torch.Size([512, 512, 3, 3])
model name:  encoder.down.2.block.1.conv1.bias and shape : torch.Size([512])
model name:  encoder.down.2.block.1.norm2.weight and shape : torch.Size([512])
model name:  encoder.down.2.block.1.norm2.bias and shape : torch.Size([512])
model name:  encoder.down.2.block.1.conv2.weight and shape : torch.Size([512, 512, 3, 3])
model name:  encoder.down.2.block.1.conv2.bias and shape : torch.Size([512])
model name:  encoder.mid.block_1.norm1.weight and shape : torch.Size([512])
model name:  encoder.mid.block_1.norm1.bias and shape : torch.Size([512])
model name:  encoder.mid.block_1.conv1.weight and shape : torch.Size([512, 512, 3, 3])
model name:  encoder.mid.block_1.conv1.bias and shape : torch.Size([512])
model name:  encoder.mid.block_1.norm2.weight and shape : torch.Size([512])
model name:  encoder.mid.block_1.norm2.bias and shape : torch.Size([512])
model name:  encoder.mid.block_1.conv2.weight and shape : torch.Size([512, 512, 3, 3])
model name:  encoder.mid.block_1.conv2.bias and shape : torch.Size([512])
model name:  encoder.mid.attn_1.norm.weight and shape : torch.Size([512])
model name:  encoder.mid.attn_1.norm.bias and shape : torch.Size([512])
model name:  encoder.mid.attn_1.q.weight and shape : torch.Size([512, 512, 1, 1])
model name:  encoder.mid.attn_1.q.bias and shape : torch.Size([512])
model name:  encoder.mid.attn_1.k.weight and shape : torch.Size([512, 512, 1, 1])
model name:  encoder.mid.attn_1.k.bias and shape : torch.Size([512])
model name:  encoder.mid.attn_1.v.weight and shape : torch.Size([512, 512, 1, 1])
model name:  encoder.mid.attn_1.v.bias and shape : torch.Size([512])
model name:  encoder.mid.attn_1.proj_out.weight and shape : torch.Size([512, 512, 1, 1])
model name:  encoder.mid.attn_1.proj_out.bias and shape : torch.Size([512])
model name:  encoder.mid.block_2.norm1.weight and shape : torch.Size([512])
model name:  encoder.mid.block_2.norm1.bias and shape : torch.Size([512])
model name:  encoder.mid.block_2.conv1.weight and shape : torch.Size([512, 512, 3, 3])
model name:  encoder.mid.block_2.conv1.bias and shape : torch.Size([512])
model name:  encoder.mid.block_2.norm2.weight and shape : torch.Size([512])
model name:  encoder.mid.block_2.norm2.bias and shape : torch.Size([512])
model name:  encoder.mid.block_2.conv2.weight and shape : torch.Size([512, 512, 3, 3])
model name:  encoder.mid.block_2.conv2.bias and shape : torch.Size([512])
model name:  encoder.norm_out.weight and shape : torch.Size([512])
model name:  encoder.norm_out.bias and shape : torch.Size([512])
model name:  encoder.conv_out.weight and shape : torch.Size([6, 512, 3, 3])
model name:  encoder.conv_out.bias and shape : torch.Size([6])
model name:  decoder.conv_in.weight and shape : torch.Size([512, 3, 3, 3])
model name:  decoder.conv_in.bias and shape : torch.Size([512])
model name:  decoder.mid.block_1.norm1.weight and shape : torch.Size([512])
model name:  decoder.mid.block_1.norm1.bias and shape : torch.Size([512])
model name:  decoder.mid.block_1.conv1.weight and shape : torch.Size([512, 512, 3, 3])
model name:  decoder.mid.block_1.conv1.bias and shape : torch.Size([512])
model name:  decoder.mid.block_1.norm2.weight and shape : torch.Size([512])
model name:  decoder.mid.block_1.norm2.bias and shape : torch.Size([512])
model name:  decoder.mid.block_1.conv2.weight and shape : torch.Size([512, 512, 3, 3])
model name:  decoder.mid.block_1.conv2.bias and shape : torch.Size([512])
model name:  decoder.mid.attn_1.norm.weight and shape : torch.Size([512])
model name:  decoder.mid.attn_1.norm.bias and shape : torch.Size([512])
model name:  decoder.mid.attn_1.q.weight and shape : torch.Size([512, 512, 1, 1])
model name:  decoder.mid.attn_1.q.bias and shape : torch.Size([512])
model name:  decoder.mid.attn_1.k.weight and shape : torch.Size([512, 512, 1, 1])
model name:  decoder.mid.attn_1.k.bias and shape : torch.Size([512])
model name:  decoder.mid.attn_1.v.weight and shape : torch.Size([512, 512, 1, 1])
model name:  decoder.mid.attn_1.v.bias and shape : torch.Size([512])
model name:  decoder.mid.attn_1.proj_out.weight and shape : torch.Size([512, 512, 1, 1])
model name:  decoder.mid.attn_1.proj_out.bias and shape : torch.Size([512])
model name:  decoder.mid.block_2.norm1.weight and shape : torch.Size([512])
model name:  decoder.mid.block_2.norm1.bias and shape : torch.Size([512])
model name:  decoder.mid.block_2.conv1.weight and shape : torch.Size([512, 512, 3, 3])
model name:  decoder.mid.block_2.conv1.bias and shape : torch.Size([512])
model name:  decoder.mid.block_2.norm2.weight and shape : torch.Size([512])
model name:  decoder.mid.block_2.norm2.bias and shape : torch.Size([512])
model name:  decoder.mid.block_2.conv2.weight and shape : torch.Size([512, 512, 3, 3])
model name:  decoder.mid.block_2.conv2.bias and shape : torch.Size([512])
model name:  decoder.up.0.block.0.norm1.weight and shape : torch.Size([256])
model name:  decoder.up.0.block.0.norm1.bias and shape : torch.Size([256])
model name:  decoder.up.0.block.0.conv1.weight and shape : torch.Size([128, 256, 3, 3])
model name:  decoder.up.0.block.0.conv1.bias and shape : torch.Size([128])
model name:  decoder.up.0.block.0.norm2.weight and shape : torch.Size([128])
model name:  decoder.up.0.block.0.norm2.bias and shape : torch.Size([128])
model name:  decoder.up.0.block.0.conv2.weight and shape : torch.Size([128, 128, 3, 3])
model name:  decoder.up.0.block.0.conv2.bias and shape : torch.Size([128])
model name:  decoder.up.0.block.0.nin_shortcut.weight and shape : torch.Size([128, 256, 1, 1])
model name:  decoder.up.0.block.0.nin_shortcut.bias and shape : torch.Size([128])
model name:  decoder.up.0.block.1.norm1.weight and shape : torch.Size([128])
model name:  decoder.up.0.block.1.norm1.bias and shape : torch.Size([128])
model name:  decoder.up.0.block.1.conv1.weight and shape : torch.Size([128, 128, 3, 3])
model name:  decoder.up.0.block.1.conv1.bias and shape : torch.Size([128])
model name:  decoder.up.0.block.1.norm2.weight and shape : torch.Size([128])
model name:  decoder.up.0.block.1.norm2.bias and shape : torch.Size([128])
model name:  decoder.up.0.block.1.conv2.weight and shape : torch.Size([128, 128, 3, 3])
model name:  decoder.up.0.block.1.conv2.bias and shape : torch.Size([128])
model name:  decoder.up.0.block.2.norm1.weight and shape : torch.Size([128])
model name:  decoder.up.0.block.2.norm1.bias and shape : torch.Size([128])
model name:  decoder.up.0.block.2.conv1.weight and shape : torch.Size([128, 128, 3, 3])
model name:  decoder.up.0.block.2.conv1.bias and shape : torch.Size([128])
model name:  decoder.up.0.block.2.norm2.weight and shape : torch.Size([128])
model name:  decoder.up.0.block.2.norm2.bias and shape : torch.Size([128])
model name:  decoder.up.0.block.2.conv2.weight and shape : torch.Size([128, 128, 3, 3])
model name:  decoder.up.0.block.2.conv2.bias and shape : torch.Size([128])
model name:  decoder.up.1.block.0.norm1.weight and shape : torch.Size([512])
model name:  decoder.up.1.block.0.norm1.bias and shape : torch.Size([512])
model name:  decoder.up.1.block.0.conv1.weight and shape : torch.Size([256, 512, 3, 3])
model name:  decoder.up.1.block.0.conv1.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.0.norm2.weight and shape : torch.Size([256])
model name:  decoder.up.1.block.0.norm2.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.0.conv2.weight and shape : torch.Size([256, 256, 3, 3])
model name:  decoder.up.1.block.0.conv2.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.0.nin_shortcut.weight and shape : torch.Size([256, 512, 1, 1])
model name:  decoder.up.1.block.0.nin_shortcut.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.1.norm1.weight and shape : torch.Size([256])
model name:  decoder.up.1.block.1.norm1.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.1.conv1.weight and shape : torch.Size([256, 256, 3, 3])
model name:  decoder.up.1.block.1.conv1.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.1.norm2.weight and shape : torch.Size([256])
model name:  decoder.up.1.block.1.norm2.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.1.conv2.weight and shape : torch.Size([256, 256, 3, 3])
model name:  decoder.up.1.block.1.conv2.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.2.norm1.weight and shape : torch.Size([256])
model name:  decoder.up.1.block.2.norm1.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.2.conv1.weight and shape : torch.Size([256, 256, 3, 3])
model name:  decoder.up.1.block.2.conv1.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.2.norm2.weight and shape : torch.Size([256])
model name:  decoder.up.1.block.2.norm2.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.2.conv2.weight and shape : torch.Size([256, 256, 3, 3])
model name:  decoder.up.1.block.2.conv2.bias and shape : torch.Size([256])
model name:  decoder.up.1.upsample.conv.weight and shape : torch.Size([256, 256, 3, 3])
model name:  decoder.up.1.upsample.conv.bias and shape : torch.Size([256])
model name:  decoder.up.2.block.0.norm1.weight and shape : torch.Size([512])
model name:  decoder.up.2.block.0.norm1.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.0.conv1.weight and shape : torch.Size([512, 512, 3, 3])
model name:  decoder.up.2.block.0.conv1.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.0.norm2.weight and shape : torch.Size([512])
model name:  decoder.up.2.block.0.norm2.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.0.conv2.weight and shape : torch.Size([512, 512, 3, 3])
model name:  decoder.up.2.block.0.conv2.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.1.norm1.weight and shape : torch.Size([512])
model name:  decoder.up.2.block.1.norm1.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.1.conv1.weight and shape : torch.Size([512, 512, 3, 3])
model name:  decoder.up.2.block.1.conv1.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.1.norm2.weight and shape : torch.Size([512])
model name:  decoder.up.2.block.1.norm2.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.1.conv2.weight and shape : torch.Size([512, 512, 3, 3])
model name:  decoder.up.2.block.1.conv2.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.2.norm1.weight and shape : torch.Size([512])
model name:  decoder.up.2.block.2.norm1.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.2.conv1.weight and shape : torch.Size([512, 512, 3, 3])
model name:  decoder.up.2.block.2.conv1.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.2.norm2.weight and shape : torch.Size([512])
model name:  decoder.up.2.block.2.norm2.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.2.conv2.weight and shape : torch.Size([512, 512, 3, 3])
model name:  decoder.up.2.block.2.conv2.bias and shape : torch.Size([512])
model name:  decoder.up.2.upsample.conv.weight and shape : torch.Size([512, 512, 3, 3])
model name:  decoder.up.2.upsample.conv.bias and shape : torch.Size([512])
model name:  decoder.norm_out.weight and shape : torch.Size([128])
model name:  decoder.norm_out.bias and shape : torch.Size([128])
model name:  decoder.conv_out.weight and shape : torch.Size([3, 128, 3, 3])
model name:  decoder.conv_out.bias and shape : torch.Size([3])
==============================================================================================================
model name:  quant_conv.weight and shape : torch.Size([6, 6, 1, 1])
model name:  quant_conv.bias and shape : torch.Size([6])
model name:  post_quant_conv.weight and shape : torch.Size([3, 3, 1, 1])
model name:  post_quant_conv.bias and shape : torch.Size([3])

================================================================================================================
model name:  loss.logvar and shape : torch.Size([])
model name:  loss.perceptual_loss.scaling_layer.shift and shape : torch.Size([1, 3, 1, 1])
model name:  loss.perceptual_loss.scaling_layer.scale and shape : torch.Size([1, 3, 1, 1])
model name:  loss.perceptual_loss.net.slice1.0.weight and shape : torch.Size([64, 3, 3, 3])
model name:  loss.perceptual_loss.net.slice1.0.bias and shape : torch.Size([64])
model name:  loss.perceptual_loss.net.slice1.2.weight and shape : torch.Size([64, 64, 3, 3])
model name:  loss.perceptual_loss.net.slice1.2.bias and shape : torch.Size([64])
model name:  loss.perceptual_loss.net.slice2.5.weight and shape : torch.Size([128, 64, 3, 3])
model name:  loss.perceptual_loss.net.slice2.5.bias and shape : torch.Size([128])
model name:  loss.perceptual_loss.net.slice2.7.weight and shape : torch.Size([128, 128, 3, 3])
model name:  loss.perceptual_loss.net.slice2.7.bias and shape : torch.Size([128])
model name:  loss.perceptual_loss.net.slice3.10.weight and shape : torch.Size([256, 128, 3, 3])
model name:  loss.perceptual_loss.net.slice3.10.bias and shape : torch.Size([256])
model name:  loss.perceptual_loss.net.slice3.12.weight and shape : torch.Size([256, 256, 3, 3])
model name:  loss.perceptual_loss.net.slice3.12.bias and shape : torch.Size([256])
model name:  loss.perceptual_loss.net.slice3.14.weight and shape : torch.Size([256, 256, 3, 3])
model name:  loss.perceptual_loss.net.slice3.14.bias and shape : torch.Size([256])
model name:  loss.perceptual_loss.net.slice4.17.weight and shape : torch.Size([512, 256, 3, 3])
model name:  loss.perceptual_loss.net.slice4.17.bias and shape : torch.Size([512])
model name:  loss.perceptual_loss.net.slice4.19.weight and shape : torch.Size([512, 512, 3, 3])
model name:  loss.perceptual_loss.net.slice4.19.bias and shape : torch.Size([512])
model name:  loss.perceptual_loss.net.slice4.21.weight and shape : torch.Size([512, 512, 3, 3])
model name:  loss.perceptual_loss.net.slice4.21.bias and shape : torch.Size([512])
model name:  loss.perceptual_loss.net.slice5.24.weight and shape : torch.Size([512, 512, 3, 3])
model name:  loss.perceptual_loss.net.slice5.24.bias and shape : torch.Size([512])
model name:  loss.perceptual_loss.net.slice5.26.weight and shape : torch.Size([512, 512, 3, 3])
model name:  loss.perceptual_loss.net.slice5.26.bias and shape : torch.Size([512])
model name:  loss.perceptual_loss.net.slice5.28.weight and shape : torch.Size([512, 512, 3, 3])
model name:  loss.perceptual_loss.net.slice5.28.bias and shape : torch.Size([512])
model name:  loss.perceptual_loss.lin0.model.1.weight and shape : torch.Size([1, 64, 1, 1])
model name:  loss.perceptual_loss.lin1.model.1.weight and shape : torch.Size([1, 128, 1, 1])
model name:  loss.perceptual_loss.lin2.model.1.weight and shape : torch.Size([1, 256, 1, 1])
model name:  loss.perceptual_loss.lin3.model.1.weight and shape : torch.Size([1, 512, 1, 1])
model name:  loss.perceptual_loss.lin4.model.1.weight and shape : torch.Size([1, 512, 1, 1])
model name:  loss.discriminator.main.0.weight and shape : torch.Size([64, 3, 4, 4])
model name:  loss.discriminator.main.0.bias and shape : torch.Size([64])
model name:  loss.discriminator.main.2.weight and shape : torch.Size([128, 64, 4, 4])
model name:  loss.discriminator.main.3.weight and shape : torch.Size([128])
model name:  loss.discriminator.main.3.bias and shape : torch.Size([128])
model name:  loss.discriminator.main.3.running_mean and shape : torch.Size([128])
model name:  loss.discriminator.main.3.running_var and shape : torch.Size([128])
model name:  loss.discriminator.main.3.num_batches_tracked and shape : torch.Size([])
model name:  loss.discriminator.main.5.weight and shape : torch.Size([256, 128, 4, 4])
model name:  loss.discriminator.main.6.weight and shape : torch.Size([256])
model name:  loss.discriminator.main.6.bias and shape : torch.Size([256])
model name:  loss.discriminator.main.6.running_mean and shape : torch.Size([256])
model name:  loss.discriminator.main.6.running_var and shape : torch.Size([256])
model name:  loss.discriminator.main.6.num_batches_tracked and shape : torch.Size([])
model name:  loss.discriminator.main.8.weight and shape : torch.Size([512, 256, 4, 4])
model name:  loss.discriminator.main.9.weight and shape : torch.Size([512])
model name:  loss.discriminator.main.9.bias and shape : torch.Size([512])
model name:  loss.discriminator.main.9.running_mean and shape : torch.Size([512])
model name:  loss.discriminator.main.9.running_var and shape : torch.Size([512])
model name:  loss.discriminator.main.9.num_batches_tracked and shape : torch.Size([])
model name:  loss.discriminator.main.11.weight and shape : torch.Size([1, 512, 4, 4])
model name:  loss.discriminator.main.11.bias and shape : torch.Size([1])


```