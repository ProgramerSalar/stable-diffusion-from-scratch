# 1. Weight size of stable-diffusion

```python
model name:  encoder.conv_in.weight and shape : torch.Size([128, 3, 3, 3])
model name:  encoder.conv_in.bias and shape : torch.Size([128])
model name:  encoder.down.0.block.0.norm1.weight and shape : torch.Size([128])
model name:  encoder.down.0.block.0.norm1.bias and shape : torch.Size([128])
model name:  encoder.down.0.block.0.conv1.weight and shape : torch.Size([128, 128, 3, 3])
model name:  encoder.down.0.block.0.conv1.bias and shape : torch.Size([128])
model name:  encoder.down.0.block.0.norm2.weight and shape : torch.Size([128])
model name:  encoder.down.0.block.0.norm2.bias and shape : torch.Size([128])
model name:  encoder.down.0.block.0.conv2.weight and shape : torch.Size([128, 128, 3, 3])
model name:  encoder.down.0.block.0.conv2.bias and shape : torch.Size([128])
model name:  encoder.down.0.block.1.norm1.weight and shape : torch.Size([128])
model name:  encoder.down.0.block.1.norm1.bias and shape : torch.Size([128])
model name:  encoder.down.0.block.1.conv1.weight and shape : torch.Size([128, 128, 3, 3])
model name:  encoder.down.0.block.1.conv1.bias and shape : torch.Size([128])
model name:  encoder.down.0.block.1.norm2.weight and shape : torch.Size([128])
model name:  encoder.down.0.block.1.norm2.bias and shape : torch.Size([128])
model name:  encoder.down.0.block.1.conv2.weight and shape : torch.Size([128, 128, 3, 3])
model name:  encoder.down.0.block.1.conv2.bias and shape : torch.Size([128])
model name:  encoder.down.0.downsample.conv.weight and shape : torch.Size([128, 128, 3, 3])
model name:  encoder.down.0.downsample.conv.bias and shape : torch.Size([128])
model name:  encoder.down.1.block.0.norm1.weight and shape : torch.Size([128])
model name:  encoder.down.1.block.0.norm1.bias and shape : torch.Size([128])
model name:  encoder.down.1.block.0.conv1.weight and shape : torch.Size([256, 128, 3, 3])
model name:  encoder.down.1.block.0.conv1.bias and shape : torch.Size([256])
model name:  encoder.down.1.block.0.norm2.weight and shape : torch.Size([256])
model name:  encoder.down.1.block.0.norm2.bias and shape : torch.Size([256])
model name:  encoder.down.1.block.0.conv2.weight and shape : torch.Size([256, 256, 3, 3])
model name:  encoder.down.1.block.0.conv2.bias and shape : torch.Size([256])
model name:  encoder.down.1.block.0.nin_shortcut.weight and shape : torch.Size([256, 128, 1, 1])
model name:  encoder.down.1.block.0.nin_shortcut.bias and shape : torch.Size([256])
model name:  encoder.down.1.block.1.norm1.weight and shape : torch.Size([256])
model name:  encoder.down.1.block.1.norm1.bias and shape : torch.Size([256])
model name:  encoder.down.1.block.1.conv1.weight and shape : torch.Size([256, 256, 3, 3])
model name:  encoder.down.1.block.1.conv1.bias and shape : torch.Size([256])
model name:  encoder.down.1.block.1.norm2.weight and shape : torch.Size([256])
model name:  encoder.down.1.block.1.norm2.bias and shape : torch.Size([256])
model name:  encoder.down.1.block.1.conv2.weight and shape : torch.Size([256, 256, 3, 3])
model name:  encoder.down.1.block.1.conv2.bias and shape : torch.Size([256])
model name:  encoder.down.1.downsample.conv.weight and shape : torch.Size([256, 256, 3, 3])
model name:  encoder.down.1.downsample.conv.bias and shape : torch.Size([256])
model name:  encoder.down.2.block.0.norm1.weight and shape : torch.Size([256])
model name:  encoder.down.2.block.0.norm1.bias and shape : torch.Size([256])
model name:  encoder.down.2.block.0.conv1.weight and shape : torch.Size([512, 256, 3, 3])
model name:  encoder.down.2.block.0.conv1.bias and shape : torch.Size([512])
model name:  encoder.down.2.block.0.norm2.weight and shape : torch.Size([512])
model name:  encoder.down.2.block.0.norm2.bias and shape : torch.Size([512])
model name:  encoder.down.2.block.0.conv2.weight and shape : torch.Size([512, 512, 3, 3])
model name:  encoder.down.2.block.0.conv2.bias and shape : torch.Size([512])
model name:  encoder.down.2.block.0.nin_shortcut.weight and shape : torch.Size([512, 256, 1, 1])
model name:  encoder.down.2.block.0.nin_shortcut.bias and shape : torch.Size([512])
model name:  encoder.down.2.block.1.norm1.weight and shape : torch.Size([512])
model name:  encoder.down.2.block.1.norm1.bias and shape : torch.Size([512])
model name:  encoder.down.2.block.1.conv1.weight and shape : torch.Size([512, 512, 3, 3])
model name:  encoder.down.2.block.1.conv1.bias and shape : torch.Size([512])
model name:  encoder.down.2.block.1.norm2.weight and shape : torch.Size([512])
model name:  encoder.down.2.block.1.norm2.bias and shape : torch.Size([512])
model name:  encoder.down.2.block.1.conv2.weight and shape : torch.Size([512, 512, 3, 3])
model name:  encoder.down.2.block.1.conv2.bias and shape : torch.Size([512])
model name:  encoder.mid.block_1.norm1.weight and shape : torch.Size([512])
model name:  encoder.mid.block_1.norm1.bias and shape : torch.Size([512])
model name:  encoder.mid.block_1.conv1.weight and shape : torch.Size([512, 512, 3, 3])
model name:  encoder.mid.block_1.conv1.bias and shape : torch.Size([512])
model name:  encoder.mid.block_1.norm2.weight and shape : torch.Size([512])
model name:  encoder.mid.block_1.norm2.bias and shape : torch.Size([512])
model name:  encoder.mid.block_1.conv2.weight and shape : torch.Size([512, 512, 3, 3])
model name:  encoder.mid.block_1.conv2.bias and shape : torch.Size([512])
model name:  encoder.mid.attn_1.norm.weight and shape : torch.Size([512])
model name:  encoder.mid.attn_1.norm.bias and shape : torch.Size([512])
model name:  encoder.mid.attn_1.q.weight and shape : torch.Size([512, 512, 1, 1])
model name:  encoder.mid.attn_1.q.bias and shape : torch.Size([512])
model name:  encoder.mid.attn_1.k.weight and shape : torch.Size([512, 512, 1, 1])
model name:  encoder.mid.attn_1.k.bias and shape : torch.Size([512])
model name:  encoder.mid.attn_1.v.weight and shape : torch.Size([512, 512, 1, 1])
model name:  encoder.mid.attn_1.v.bias and shape : torch.Size([512])
model name:  encoder.mid.attn_1.proj_out.weight and shape : torch.Size([512, 512, 1, 1])
model name:  encoder.mid.attn_1.proj_out.bias and shape : torch.Size([512])
model name:  encoder.mid.block_2.norm1.weight and shape : torch.Size([512])
model name:  encoder.mid.block_2.norm1.bias and shape : torch.Size([512])
model name:  encoder.mid.block_2.conv1.weight and shape : torch.Size([512, 512, 3, 3])
model name:  encoder.mid.block_2.conv1.bias and shape : torch.Size([512])
model name:  encoder.mid.block_2.norm2.weight and shape : torch.Size([512])
model name:  encoder.mid.block_2.norm2.bias and shape : torch.Size([512])
model name:  encoder.mid.block_2.conv2.weight and shape : torch.Size([512, 512, 3, 3])
model name:  encoder.mid.block_2.conv2.bias and shape : torch.Size([512])
model name:  encoder.norm_out.weight and shape : torch.Size([512])
model name:  encoder.norm_out.bias and shape : torch.Size([512])
model name:  encoder.conv_out.weight and shape : torch.Size([6, 512, 3, 3])
model name:  encoder.conv_out.bias and shape : torch.Size([6])
model name:  decoder.conv_in.weight and shape : torch.Size([512, 3, 3, 3])
model name:  decoder.conv_in.bias and shape : torch.Size([512])
model name:  decoder.mid.block_1.norm1.weight and shape : torch.Size([512])
model name:  decoder.mid.block_1.norm1.bias and shape : torch.Size([512])
model name:  decoder.mid.block_1.conv1.weight and shape : torch.Size([512, 512, 3, 3])
model name:  decoder.mid.block_1.conv1.bias and shape : torch.Size([512])
model name:  decoder.mid.block_1.norm2.weight and shape : torch.Size([512])
model name:  decoder.mid.block_1.norm2.bias and shape : torch.Size([512])
model name:  decoder.mid.block_1.conv2.weight and shape : torch.Size([512, 512, 3, 3])
model name:  decoder.mid.block_1.conv2.bias and shape : torch.Size([512])
model name:  decoder.mid.attn_1.norm.weight and shape : torch.Size([512])
model name:  decoder.mid.attn_1.norm.bias and shape : torch.Size([512])
model name:  decoder.mid.attn_1.q.weight and shape : torch.Size([512, 512, 1, 1])
model name:  decoder.mid.attn_1.q.bias and shape : torch.Size([512])
model name:  decoder.mid.attn_1.k.weight and shape : torch.Size([512, 512, 1, 1])
model name:  decoder.mid.attn_1.k.bias and shape : torch.Size([512])
model name:  decoder.mid.attn_1.v.weight and shape : torch.Size([512, 512, 1, 1])
model name:  decoder.mid.attn_1.v.bias and shape : torch.Size([512])
model name:  decoder.mid.attn_1.proj_out.weight and shape : torch.Size([512, 512, 1, 1])
model name:  decoder.mid.attn_1.proj_out.bias and shape : torch.Size([512])
model name:  decoder.mid.block_2.norm1.weight and shape : torch.Size([512])
model name:  decoder.mid.block_2.norm1.bias and shape : torch.Size([512])
model name:  decoder.mid.block_2.conv1.weight and shape : torch.Size([512, 512, 3, 3])
model name:  decoder.mid.block_2.conv1.bias and shape : torch.Size([512])
model name:  decoder.mid.block_2.norm2.weight and shape : torch.Size([512])
model name:  decoder.mid.block_2.norm2.bias and shape : torch.Size([512])
model name:  decoder.mid.block_2.conv2.weight and shape : torch.Size([512, 512, 3, 3])
model name:  decoder.mid.block_2.conv2.bias and shape : torch.Size([512])
model name:  decoder.up.0.block.0.norm1.weight and shape : torch.Size([256])
model name:  decoder.up.0.block.0.norm1.bias and shape : torch.Size([256])
model name:  decoder.up.0.block.0.conv1.weight and shape : torch.Size([128, 256, 3, 3])
model name:  decoder.up.0.block.0.conv1.bias and shape : torch.Size([128])
model name:  decoder.up.0.block.0.norm2.weight and shape : torch.Size([128])
model name:  decoder.up.0.block.0.norm2.bias and shape : torch.Size([128])
model name:  decoder.up.0.block.0.conv2.weight and shape : torch.Size([128, 128, 3, 3])
model name:  decoder.up.0.block.0.conv2.bias and shape : torch.Size([128])
model name:  decoder.up.0.block.0.nin_shortcut.weight and shape : torch.Size([128, 256, 1, 1])
model name:  decoder.up.0.block.0.nin_shortcut.bias and shape : torch.Size([128])
model name:  decoder.up.0.block.1.norm1.weight and shape : torch.Size([128])
model name:  decoder.up.0.block.1.norm1.bias and shape : torch.Size([128])
model name:  decoder.up.0.block.1.conv1.weight and shape : torch.Size([128, 128, 3, 3])
model name:  decoder.up.0.block.1.conv1.bias and shape : torch.Size([128])
model name:  decoder.up.0.block.1.norm2.weight and shape : torch.Size([128])
model name:  decoder.up.0.block.1.norm2.bias and shape : torch.Size([128])
model name:  decoder.up.0.block.1.conv2.weight and shape : torch.Size([128, 128, 3, 3])
model name:  decoder.up.0.block.1.conv2.bias and shape : torch.Size([128])
model name:  decoder.up.0.block.2.norm1.weight and shape : torch.Size([128])
model name:  decoder.up.0.block.2.norm1.bias and shape : torch.Size([128])
model name:  decoder.up.0.block.2.conv1.weight and shape : torch.Size([128, 128, 3, 3])
model name:  decoder.up.0.block.2.conv1.bias and shape : torch.Size([128])
model name:  decoder.up.0.block.2.norm2.weight and shape : torch.Size([128])
model name:  decoder.up.0.block.2.norm2.bias and shape : torch.Size([128])
model name:  decoder.up.0.block.2.conv2.weight and shape : torch.Size([128, 128, 3, 3])
model name:  decoder.up.0.block.2.conv2.bias and shape : torch.Size([128])
model name:  decoder.up.1.block.0.norm1.weight and shape : torch.Size([512])
model name:  decoder.up.1.block.0.norm1.bias and shape : torch.Size([512])
model name:  decoder.up.1.block.0.conv1.weight and shape : torch.Size([256, 512, 3, 3])
model name:  decoder.up.1.block.0.conv1.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.0.norm2.weight and shape : torch.Size([256])
model name:  decoder.up.1.block.0.norm2.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.0.conv2.weight and shape : torch.Size([256, 256, 3, 3])
model name:  decoder.up.1.block.0.conv2.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.0.nin_shortcut.weight and shape : torch.Size([256, 512, 1, 1])
model name:  decoder.up.1.block.0.nin_shortcut.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.1.norm1.weight and shape : torch.Size([256])
model name:  decoder.up.1.block.1.norm1.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.1.conv1.weight and shape : torch.Size([256, 256, 3, 3])
model name:  decoder.up.1.block.1.conv1.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.1.norm2.weight and shape : torch.Size([256])
model name:  decoder.up.1.block.1.norm2.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.1.conv2.weight and shape : torch.Size([256, 256, 3, 3])
model name:  decoder.up.1.block.1.conv2.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.2.norm1.weight and shape : torch.Size([256])
model name:  decoder.up.1.block.2.norm1.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.2.conv1.weight and shape : torch.Size([256, 256, 3, 3])
model name:  decoder.up.1.block.2.conv1.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.2.norm2.weight and shape : torch.Size([256])
model name:  decoder.up.1.block.2.norm2.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.2.conv2.weight and shape : torch.Size([256, 256, 3, 3])
model name:  decoder.up.1.block.2.conv2.bias and shape : torch.Size([256])
model name:  decoder.up.1.upsample.conv.weight and shape : torch.Size([256, 256, 3, 3])
model name:  decoder.up.1.upsample.conv.bias and shape : torch.Size([256])
model name:  decoder.up.2.block.0.norm1.weight and shape : torch.Size([512])
model name:  decoder.up.2.block.0.norm1.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.0.conv1.weight and shape : torch.Size([512, 512, 3, 3])
model name:  decoder.up.2.block.0.conv1.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.0.norm2.weight and shape : torch.Size([512])
model name:  decoder.up.2.block.0.norm2.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.0.conv2.weight and shape : torch.Size([512, 512, 3, 3])
model name:  decoder.up.2.block.0.conv2.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.1.norm1.weight and shape : torch.Size([512])
model name:  decoder.up.2.block.1.norm1.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.1.conv1.weight and shape : torch.Size([512, 512, 3, 3])
model name:  decoder.up.2.block.1.conv1.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.1.norm2.weight and shape : torch.Size([512])
model name:  decoder.up.2.block.1.norm2.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.1.conv2.weight and shape : torch.Size([512, 512, 3, 3])
model name:  decoder.up.2.block.1.conv2.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.2.norm1.weight and shape : torch.Size([512])
model name:  decoder.up.2.block.2.norm1.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.2.conv1.weight and shape : torch.Size([512, 512, 3, 3])
model name:  decoder.up.2.block.2.conv1.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.2.norm2.weight and shape : torch.Size([512])
model name:  decoder.up.2.block.2.norm2.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.2.conv2.weight and shape : torch.Size([512, 512, 3, 3])
model name:  decoder.up.2.block.2.conv2.bias and shape : torch.Size([512])
model name:  decoder.up.2.upsample.conv.weight and shape : torch.Size([512, 512, 3, 3])
model name:  decoder.up.2.upsample.conv.bias and shape : torch.Size([512])
model name:  decoder.norm_out.weight and shape : torch.Size([128])
model name:  decoder.norm_out.bias and shape : torch.Size([128])
model name:  decoder.conv_out.weight and shape : torch.Size([3, 128, 3, 3])
model name:  decoder.conv_out.bias and shape : torch.Size([3])
model name:  loss.logvar and shape : torch.Size([])
model name:  loss.perceptual_loss.scaling_layer.shift and shape : torch.Size([1, 3, 1, 1])
model name:  loss.perceptual_loss.scaling_layer.scale and shape : torch.Size([1, 3, 1, 1])
model name:  loss.perceptual_loss.net.slice1.0.weight and shape : torch.Size([64, 3, 3, 3])
model name:  loss.perceptual_loss.net.slice1.0.bias and shape : torch.Size([64])
model name:  loss.perceptual_loss.net.slice1.2.weight and shape : torch.Size([64, 64, 3, 3])
model name:  loss.perceptual_loss.net.slice1.2.bias and shape : torch.Size([64])
model name:  loss.perceptual_loss.net.slice2.5.weight and shape : torch.Size([128, 64, 3, 3])
model name:  loss.perceptual_loss.net.slice2.5.bias and shape : torch.Size([128])
model name:  loss.perceptual_loss.net.slice2.7.weight and shape : torch.Size([128, 128, 3, 3])
model name:  loss.perceptual_loss.net.slice2.7.bias and shape : torch.Size([128])
model name:  loss.perceptual_loss.net.slice3.10.weight and shape : torch.Size([256, 128, 3, 3])
model name:  loss.perceptual_loss.net.slice3.10.bias and shape : torch.Size([256])
model name:  loss.perceptual_loss.net.slice3.12.weight and shape : torch.Size([256, 256, 3, 3])
model name:  loss.perceptual_loss.net.slice3.12.bias and shape : torch.Size([256])
model name:  loss.perceptual_loss.net.slice3.14.weight and shape : torch.Size([256, 256, 3, 3])
model name:  loss.perceptual_loss.net.slice3.14.bias and shape : torch.Size([256])
model name:  loss.perceptual_loss.net.slice4.17.weight and shape : torch.Size([512, 256, 3, 3])
model name:  loss.perceptual_loss.net.slice4.17.bias and shape : torch.Size([512])
model name:  loss.perceptual_loss.net.slice4.19.weight and shape : torch.Size([512, 512, 3, 3])
model name:  loss.perceptual_loss.net.slice4.19.bias and shape : torch.Size([512])
model name:  loss.perceptual_loss.net.slice4.21.weight and shape : torch.Size([512, 512, 3, 3])
model name:  loss.perceptual_loss.net.slice4.21.bias and shape : torch.Size([512])
model name:  loss.perceptual_loss.net.slice5.24.weight and shape : torch.Size([512, 512, 3, 3])
model name:  loss.perceptual_loss.net.slice5.24.bias and shape : torch.Size([512])
model name:  loss.perceptual_loss.net.slice5.26.weight and shape : torch.Size([512, 512, 3, 3])
model name:  loss.perceptual_loss.net.slice5.26.bias and shape : torch.Size([512])
model name:  loss.perceptual_loss.net.slice5.28.weight and shape : torch.Size([512, 512, 3, 3])
model name:  loss.perceptual_loss.net.slice5.28.bias and shape : torch.Size([512])
model name:  loss.perceptual_loss.lin0.model.1.weight and shape : torch.Size([1, 64, 1, 1])
model name:  loss.perceptual_loss.lin1.model.1.weight and shape : torch.Size([1, 128, 1, 1])
model name:  loss.perceptual_loss.lin2.model.1.weight and shape : torch.Size([1, 256, 1, 1])
model name:  loss.perceptual_loss.lin3.model.1.weight and shape : torch.Size([1, 512, 1, 1])
model name:  loss.perceptual_loss.lin4.model.1.weight and shape : torch.Size([1, 512, 1, 1])
model name:  loss.discriminator.main.0.weight and shape : torch.Size([64, 3, 4, 4])
model name:  loss.discriminator.main.0.bias and shape : torch.Size([64])
model name:  loss.discriminator.main.2.weight and shape : torch.Size([128, 64, 4, 4])
model name:  loss.discriminator.main.3.weight and shape : torch.Size([128])
model name:  loss.discriminator.main.3.bias and shape : torch.Size([128])
model name:  loss.discriminator.main.3.running_mean and shape : torch.Size([128])
model name:  loss.discriminator.main.3.running_var and shape : torch.Size([128])
model name:  loss.discriminator.main.3.num_batches_tracked and shape : torch.Size([])
model name:  loss.discriminator.main.5.weight and shape : torch.Size([256, 128, 4, 4])
model name:  loss.discriminator.main.6.weight and shape : torch.Size([256])
model name:  loss.discriminator.main.6.bias and shape : torch.Size([256])
model name:  loss.discriminator.main.6.running_mean and shape : torch.Size([256])
model name:  loss.discriminator.main.6.running_var and shape : torch.Size([256])
model name:  loss.discriminator.main.6.num_batches_tracked and shape : torch.Size([])
model name:  loss.discriminator.main.8.weight and shape : torch.Size([512, 256, 4, 4])
model name:  loss.discriminator.main.9.weight and shape : torch.Size([512])
model name:  loss.discriminator.main.9.bias and shape : torch.Size([512])
model name:  loss.discriminator.main.9.running_mean and shape : torch.Size([512])
model name:  loss.discriminator.main.9.running_var and shape : torch.Size([512])
model name:  loss.discriminator.main.9.num_batches_tracked and shape : torch.Size([])
model name:  loss.discriminator.main.11.weight and shape : torch.Size([1, 512, 4, 4])
model name:  loss.discriminator.main.11.bias and shape : torch.Size([1])
model name:  quant_conv.weight and shape : torch.Size([6, 6, 1, 1])
model name:  quant_conv.bias and shape : torch.Size([6])
model name:  post_quant_conv.weight and shape : torch.Size([3, 3, 1, 1])
model name:  post_quant_conv.bias and shape : torch.Size([3])


```

# 2. This shape of weight to me epoch==10
```
model name:  encoder.conv_in.weight and shape : torch.Size([128, 3, 3, 3])
model name:  encoder.conv_in.bias and shape : torch.Size([128])
model name:  encoder.down.0.block.0.norm1.weight and shape : torch.Size([128])
model name:  encoder.down.0.block.0.norm1.bias and shape : torch.Size([128])
model name:  encoder.down.0.block.0.conv1.weight and shape : torch.Size([128, 128, 3, 3])
model name:  encoder.down.0.block.0.conv1.bias and shape : torch.Size([128])
model name:  encoder.down.0.block.0.norm2.weight and shape : torch.Size([128])
model name:  encoder.down.0.block.0.norm2.bias and shape : torch.Size([128])
model name:  encoder.down.0.block.0.conv2.weight and shape : torch.Size([128, 128, 3, 3])
model name:  encoder.down.0.block.0.conv2.bias and shape : torch.Size([128])
model name:  encoder.down.0.block.1.norm1.weight and shape : torch.Size([128])
model name:  encoder.down.0.block.1.norm1.bias and shape : torch.Size([128])
model name:  encoder.down.0.block.1.conv1.weight and shape : torch.Size([128, 128, 3, 3])
model name:  encoder.down.0.block.1.conv1.bias and shape : torch.Size([128])
model name:  encoder.down.0.block.1.norm2.weight and shape : torch.Size([128])
model name:  encoder.down.0.block.1.norm2.bias and shape : torch.Size([128])
model name:  encoder.down.0.block.1.conv2.weight and shape : torch.Size([128, 128, 3, 3])
model name:  encoder.down.0.block.1.conv2.bias and shape : torch.Size([128])
model name:  encoder.down.0.downsample.conv.weight and shape : torch.Size([128, 128, 3, 3])
model name:  encoder.down.0.downsample.conv.bias and shape : torch.Size([128])
model name:  encoder.down.1.block.0.norm1.weight and shape : torch.Size([128])
model name:  encoder.down.1.block.0.norm1.bias and shape : torch.Size([128])
model name:  encoder.down.1.block.0.conv1.weight and shape : torch.Size([256, 128, 3, 3])
model name:  encoder.down.1.block.0.conv1.bias and shape : torch.Size([256])
model name:  encoder.down.1.block.0.norm2.weight and shape : torch.Size([256])
model name:  encoder.down.1.block.0.norm2.bias and shape : torch.Size([256])
model name:  encoder.down.1.block.0.conv2.weight and shape : torch.Size([256, 256, 3, 3])
model name:  encoder.down.1.block.0.conv2.bias and shape : torch.Size([256])
model name:  encoder.down.1.block.0.nin_shortcut.weight and shape : torch.Size([256, 128, 1, 1])
model name:  encoder.down.1.block.0.nin_shortcut.bias and shape : torch.Size([256])
model name:  encoder.down.1.block.1.norm1.weight and shape : torch.Size([256])
model name:  encoder.down.1.block.1.norm1.bias and shape : torch.Size([256])
model name:  encoder.down.1.block.1.conv1.weight and shape : torch.Size([256, 256, 3, 3])
model name:  encoder.down.1.block.1.conv1.bias and shape : torch.Size([256])
model name:  encoder.down.1.block.1.norm2.weight and shape : torch.Size([256])
model name:  encoder.down.1.block.1.norm2.bias and shape : torch.Size([256])
model name:  encoder.down.1.block.1.conv2.weight and shape : torch.Size([256, 256, 3, 3])
model name:  encoder.down.1.block.1.conv2.bias and shape : torch.Size([256])
model name:  encoder.down.1.downsample.conv.weight and shape : torch.Size([256, 256, 3, 3])
model name:  encoder.down.1.downsample.conv.bias and shape : torch.Size([256])
model name:  encoder.down.2.block.0.norm1.weight and shape : torch.Size([256])
model name:  encoder.down.2.block.0.norm1.bias and shape : torch.Size([256])
model name:  encoder.down.2.block.0.conv1.weight and shape : torch.Size([512, 256, 3, 3])
model name:  encoder.down.2.block.0.conv1.bias and shape : torch.Size([512])
model name:  encoder.down.2.block.0.norm2.weight and shape : torch.Size([512])
model name:  encoder.down.2.block.0.norm2.bias and shape : torch.Size([512])
model name:  encoder.down.2.block.0.conv2.weight and shape : torch.Size([512, 512, 3, 3])
model name:  encoder.down.2.block.0.conv2.bias and shape : torch.Size([512])
model name:  encoder.down.2.block.0.nin_shortcut.weight and shape : torch.Size([512, 256, 1, 1])
model name:  encoder.down.2.block.0.nin_shortcut.bias and shape : torch.Size([512])
model name:  encoder.down.2.block.1.norm1.weight and shape : torch.Size([512])
model name:  encoder.down.2.block.1.norm1.bias and shape : torch.Size([512])
model name:  encoder.down.2.block.1.conv1.weight and shape : torch.Size([512, 512, 3, 3])
model name:  encoder.down.2.block.1.conv1.bias and shape : torch.Size([512])
model name:  encoder.down.2.block.1.norm2.weight and shape : torch.Size([512])
model name:  encoder.down.2.block.1.norm2.bias and shape : torch.Size([512])
model name:  encoder.down.2.block.1.conv2.weight and shape : torch.Size([512, 512, 3, 3])
model name:  encoder.down.2.block.1.conv2.bias and shape : torch.Size([512])
model name:  encoder.mid.block_1.norm1.weight and shape : torch.Size([512])
model name:  encoder.mid.block_1.norm1.bias and shape : torch.Size([512])
model name:  encoder.mid.block_1.conv1.weight and shape : torch.Size([512, 512, 3, 3])
model name:  encoder.mid.block_1.conv1.bias and shape : torch.Size([512])
model name:  encoder.mid.block_1.norm2.weight and shape : torch.Size([512])
model name:  encoder.mid.block_1.norm2.bias and shape : torch.Size([512])
model name:  encoder.mid.block_1.conv2.weight and shape : torch.Size([512, 512, 3, 3])
model name:  encoder.mid.block_1.conv2.bias and shape : torch.Size([512])
model name:  encoder.mid.attn_1.norm.weight and shape : torch.Size([512])
model name:  encoder.mid.attn_1.norm.bias and shape : torch.Size([512])
model name:  encoder.mid.attn_1.q.weight and shape : torch.Size([512, 512, 1, 1])
model name:  encoder.mid.attn_1.q.bias and shape : torch.Size([512])
model name:  encoder.mid.attn_1.k.weight and shape : torch.Size([512, 512, 1, 1])
model name:  encoder.mid.attn_1.k.bias and shape : torch.Size([512])
model name:  encoder.mid.attn_1.v.weight and shape : torch.Size([512, 512, 1, 1])
model name:  encoder.mid.attn_1.v.bias and shape : torch.Size([512])
model name:  encoder.mid.attn_1.proj_out.weight and shape : torch.Size([512, 512, 1, 1])
model name:  encoder.mid.attn_1.proj_out.bias and shape : torch.Size([512])
model name:  encoder.mid.block_2.norm1.weight and shape : torch.Size([512])
model name:  encoder.mid.block_2.norm1.bias and shape : torch.Size([512])
model name:  encoder.mid.block_2.conv1.weight and shape : torch.Size([512, 512, 3, 3])
model name:  encoder.mid.block_2.conv1.bias and shape : torch.Size([512])
model name:  encoder.mid.block_2.norm2.weight and shape : torch.Size([512])
model name:  encoder.mid.block_2.norm2.bias and shape : torch.Size([512])
model name:  encoder.mid.block_2.conv2.weight and shape : torch.Size([512, 512, 3, 3])
model name:  encoder.mid.block_2.conv2.bias and shape : torch.Size([512])
model name:  encoder.norm_out.weight and shape : torch.Size([512])
model name:  encoder.norm_out.bias and shape : torch.Size([512])
model name:  encoder.conv_out.weight and shape : torch.Size([6, 512, 3, 3])
model name:  encoder.conv_out.bias and shape : torch.Size([6])
model name:  decoder.conv_in.weight and shape : torch.Size([512, 3, 3, 3])
model name:  decoder.conv_in.bias and shape : torch.Size([512])
model name:  decoder.mid.block_1.norm1.weight and shape : torch.Size([512])
model name:  decoder.mid.block_1.norm1.bias and shape : torch.Size([512])
model name:  decoder.mid.block_1.conv1.weight and shape : torch.Size([512, 512, 3, 3])
model name:  decoder.mid.block_1.conv1.bias and shape : torch.Size([512])
model name:  decoder.mid.block_1.norm2.weight and shape : torch.Size([512])
model name:  decoder.mid.block_1.norm2.bias and shape : torch.Size([512])
model name:  decoder.mid.block_1.conv2.weight and shape : torch.Size([512, 512, 3, 3])
model name:  decoder.mid.block_1.conv2.bias and shape : torch.Size([512])
model name:  decoder.mid.attn_1.norm.weight and shape : torch.Size([512])
model name:  decoder.mid.attn_1.norm.bias and shape : torch.Size([512])
model name:  decoder.mid.attn_1.q.weight and shape : torch.Size([512, 512, 1, 1])
model name:  decoder.mid.attn_1.q.bias and shape : torch.Size([512])
model name:  decoder.mid.attn_1.k.weight and shape : torch.Size([512, 512, 1, 1])
model name:  decoder.mid.attn_1.k.bias and shape : torch.Size([512])
model name:  decoder.mid.attn_1.v.weight and shape : torch.Size([512, 512, 1, 1])
model name:  decoder.mid.attn_1.v.bias and shape : torch.Size([512])
model name:  decoder.mid.attn_1.proj_out.weight and shape : torch.Size([512, 512, 1, 1])
model name:  decoder.mid.attn_1.proj_out.bias and shape : torch.Size([512])
model name:  decoder.mid.block_2.norm1.weight and shape : torch.Size([512])
model name:  decoder.mid.block_2.norm1.bias and shape : torch.Size([512])
model name:  decoder.mid.block_2.conv1.weight and shape : torch.Size([512, 512, 3, 3])
model name:  decoder.mid.block_2.conv1.bias and shape : torch.Size([512])
model name:  decoder.mid.block_2.norm2.weight and shape : torch.Size([512])
model name:  decoder.mid.block_2.norm2.bias and shape : torch.Size([512])
model name:  decoder.mid.block_2.conv2.weight and shape : torch.Size([512, 512, 3, 3])
model name:  decoder.mid.block_2.conv2.bias and shape : torch.Size([512])
model name:  decoder.up.0.block.0.norm1.weight and shape : torch.Size([256])
model name:  decoder.up.0.block.0.norm1.bias and shape : torch.Size([256])
model name:  decoder.up.0.block.0.conv1.weight and shape : torch.Size([128, 256, 3, 3])
model name:  decoder.up.0.block.0.conv1.bias and shape : torch.Size([128])
model name:  decoder.up.0.block.0.norm2.weight and shape : torch.Size([128])
model name:  decoder.up.0.block.0.norm2.bias and shape : torch.Size([128])
model name:  decoder.up.0.block.0.conv2.weight and shape : torch.Size([128, 128, 3, 3])
model name:  decoder.up.0.block.0.conv2.bias and shape : torch.Size([128])
model name:  decoder.up.0.block.0.nin_shortcut.weight and shape : torch.Size([128, 256, 1, 1])
model name:  decoder.up.0.block.0.nin_shortcut.bias and shape : torch.Size([128])
model name:  decoder.up.0.block.1.norm1.weight and shape : torch.Size([128])
model name:  decoder.up.0.block.1.norm1.bias and shape : torch.Size([128])
model name:  decoder.up.0.block.1.conv1.weight and shape : torch.Size([128, 128, 3, 3])
model name:  decoder.up.0.block.1.conv1.bias and shape : torch.Size([128])
model name:  decoder.up.0.block.1.norm2.weight and shape : torch.Size([128])
model name:  decoder.up.0.block.1.norm2.bias and shape : torch.Size([128])
model name:  decoder.up.0.block.1.conv2.weight and shape : torch.Size([128, 128, 3, 3])
model name:  decoder.up.0.block.1.conv2.bias and shape : torch.Size([128])
model name:  decoder.up.0.block.2.norm1.weight and shape : torch.Size([128])
model name:  decoder.up.0.block.2.norm1.bias and shape : torch.Size([128])
model name:  decoder.up.0.block.2.conv1.weight and shape : torch.Size([128, 128, 3, 3])
model name:  decoder.up.0.block.2.conv1.bias and shape : torch.Size([128])
model name:  decoder.up.0.block.2.norm2.weight and shape : torch.Size([128])
model name:  decoder.up.0.block.2.norm2.bias and shape : torch.Size([128])
model name:  decoder.up.0.block.2.conv2.weight and shape : torch.Size([128, 128, 3, 3])
model name:  decoder.up.0.block.2.conv2.bias and shape : torch.Size([128])
model name:  decoder.up.1.block.0.norm1.weight and shape : torch.Size([512])
model name:  decoder.up.1.block.0.norm1.bias and shape : torch.Size([512])
model name:  decoder.up.1.block.0.conv1.weight and shape : torch.Size([256, 512, 3, 3])
model name:  decoder.up.1.block.0.conv1.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.0.norm2.weight and shape : torch.Size([256])
model name:  decoder.up.1.block.0.norm2.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.0.conv2.weight and shape : torch.Size([256, 256, 3, 3])
model name:  decoder.up.1.block.0.conv2.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.0.nin_shortcut.weight and shape : torch.Size([256, 512, 1, 1])
model name:  decoder.up.1.block.0.nin_shortcut.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.1.norm1.weight and shape : torch.Size([256])
model name:  decoder.up.1.block.1.norm1.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.1.conv1.weight and shape : torch.Size([256, 256, 3, 3])
model name:  decoder.up.1.block.1.conv1.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.1.norm2.weight and shape : torch.Size([256])
model name:  decoder.up.1.block.1.norm2.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.1.conv2.weight and shape : torch.Size([256, 256, 3, 3])
model name:  decoder.up.1.block.1.conv2.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.2.norm1.weight and shape : torch.Size([256])
model name:  decoder.up.1.block.2.norm1.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.2.conv1.weight and shape : torch.Size([256, 256, 3, 3])
model name:  decoder.up.1.block.2.conv1.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.2.norm2.weight and shape : torch.Size([256])
model name:  decoder.up.1.block.2.norm2.bias and shape : torch.Size([256])
model name:  decoder.up.1.block.2.conv2.weight and shape : torch.Size([256, 256, 3, 3])
model name:  decoder.up.1.block.2.conv2.bias and shape : torch.Size([256])
model name:  decoder.up.1.upsample.conv.weight and shape : torch.Size([256, 256, 3, 3])
model name:  decoder.up.1.upsample.conv.bias and shape : torch.Size([256])
model name:  decoder.up.2.block.0.norm1.weight and shape : torch.Size([512])
model name:  decoder.up.2.block.0.norm1.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.0.conv1.weight and shape : torch.Size([512, 512, 3, 3])
model name:  decoder.up.2.block.0.conv1.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.0.norm2.weight and shape : torch.Size([512])
model name:  decoder.up.2.block.0.norm2.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.0.conv2.weight and shape : torch.Size([512, 512, 3, 3])
model name:  decoder.up.2.block.0.conv2.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.1.norm1.weight and shape : torch.Size([512])
model name:  decoder.up.2.block.1.norm1.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.1.conv1.weight and shape : torch.Size([512, 512, 3, 3])
model name:  decoder.up.2.block.1.conv1.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.1.norm2.weight and shape : torch.Size([512])
model name:  decoder.up.2.block.1.norm2.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.1.conv2.weight and shape : torch.Size([512, 512, 3, 3])
model name:  decoder.up.2.block.1.conv2.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.2.norm1.weight and shape : torch.Size([512])
model name:  decoder.up.2.block.2.norm1.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.2.conv1.weight and shape : torch.Size([512, 512, 3, 3])
model name:  decoder.up.2.block.2.conv1.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.2.norm2.weight and shape : torch.Size([512])
model name:  decoder.up.2.block.2.norm2.bias and shape : torch.Size([512])
model name:  decoder.up.2.block.2.conv2.weight and shape : torch.Size([512, 512, 3, 3])
model name:  decoder.up.2.block.2.conv2.bias and shape : torch.Size([512])
model name:  decoder.up.2.upsample.conv.weight and shape : torch.Size([512, 512, 3, 3])
model name:  decoder.up.2.upsample.conv.bias and shape : torch.Size([512])
model name:  decoder.norm_out.weight and shape : torch.Size([128])
model name:  decoder.norm_out.bias and shape : torch.Size([128])
model name:  decoder.conv_out.weight and shape : torch.Size([3, 128, 3, 3])
model name:  decoder.conv_out.bias and shape : torch.Size([3])
model name:  quant_conv.weight and shape : torch.Size([6, 6, 1, 1])
model name:  quant_conv.bias and shape : torch.Size([6])
model name:  post_quant_conv.weight and shape : torch.Size([3, 3, 1, 1])
model name:  post_quant_conv.bias and shape : torch.Size([3])
model name:  loss.logvar and shape : torch.Size([])
model name:  loss.perceptual_loss.scaling_layer.shift and shape : torch.Size([1, 3, 1, 1])
model name:  loss.perceptual_loss.scaling_layer.scale and shape : torch.Size([1, 3, 1, 1])
model name:  loss.perceptual_loss.net.slice1.0.weight and shape : torch.Size([64, 3, 3, 3])
model name:  loss.perceptual_loss.net.slice1.0.bias and shape : torch.Size([64])
model name:  loss.perceptual_loss.net.slice1.2.weight and shape : torch.Size([64, 64, 3, 3])
model name:  loss.perceptual_loss.net.slice1.2.bias and shape : torch.Size([64])
model name:  loss.perceptual_loss.net.slice2.5.weight and shape : torch.Size([128, 64, 3, 3])
model name:  loss.perceptual_loss.net.slice2.5.bias and shape : torch.Size([128])
model name:  loss.perceptual_loss.net.slice2.7.weight and shape : torch.Size([128, 128, 3, 3])
model name:  loss.perceptual_loss.net.slice2.7.bias and shape : torch.Size([128])
model name:  loss.perceptual_loss.net.slice3.10.weight and shape : torch.Size([256, 128, 3, 3])
model name:  loss.perceptual_loss.net.slice3.10.bias and shape : torch.Size([256])
model name:  loss.perceptual_loss.net.slice3.12.weight and shape : torch.Size([256, 256, 3, 3])
model name:  loss.perceptual_loss.net.slice3.12.bias and shape : torch.Size([256])
model name:  loss.perceptual_loss.net.slice3.14.weight and shape : torch.Size([256, 256, 3, 3])
model name:  loss.perceptual_loss.net.slice3.14.bias and shape : torch.Size([256])
model name:  loss.perceptual_loss.net.slice4.17.weight and shape : torch.Size([512, 256, 3, 3])
model name:  loss.perceptual_loss.net.slice4.17.bias and shape : torch.Size([512])
model name:  loss.perceptual_loss.net.slice4.19.weight and shape : torch.Size([512, 512, 3, 3])
model name:  loss.perceptual_loss.net.slice4.19.bias and shape : torch.Size([512])
model name:  loss.perceptual_loss.net.slice4.21.weight and shape : torch.Size([512, 512, 3, 3])
model name:  loss.perceptual_loss.net.slice4.21.bias and shape : torch.Size([512])
model name:  loss.perceptual_loss.net.slice5.24.weight and shape : torch.Size([512, 512, 3, 3])
model name:  loss.perceptual_loss.net.slice5.24.bias and shape : torch.Size([512])
model name:  loss.perceptual_loss.net.slice5.26.weight and shape : torch.Size([512, 512, 3, 3])
model name:  loss.perceptual_loss.net.slice5.26.bias and shape : torch.Size([512])
model name:  loss.perceptual_loss.net.slice5.28.weight and shape : torch.Size([512, 512, 3, 3])
model name:  loss.perceptual_loss.net.slice5.28.bias and shape : torch.Size([512])
model name:  loss.perceptual_loss.lin0.model.1.weight and shape : torch.Size([1, 64, 1, 1])
model name:  loss.perceptual_loss.lin1.model.1.weight and shape : torch.Size([1, 128, 1, 1])
model name:  loss.perceptual_loss.lin2.model.1.weight and shape : torch.Size([1, 256, 1, 1])
model name:  loss.perceptual_loss.lin3.model.1.weight and shape : torch.Size([1, 512, 1, 1])
model name:  loss.perceptual_loss.lin4.model.1.weight and shape : torch.Size([1, 512, 1, 1])
model name:  loss.discriminator.main.0.weight and shape : torch.Size([64, 3, 4, 4])
model name:  loss.discriminator.main.0.bias and shape : torch.Size([64])
model name:  loss.discriminator.main.2.weight and shape : torch.Size([128, 64, 4, 4])
model name:  loss.discriminator.main.3.weight and shape : torch.Size([128])
model name:  loss.discriminator.main.3.bias and shape : torch.Size([128])
model name:  loss.discriminator.main.3.running_mean and shape : torch.Size([128])
model name:  loss.discriminator.main.3.running_var and shape : torch.Size([128])
model name:  loss.discriminator.main.3.num_batches_tracked and shape : torch.Size([])
model name:  loss.discriminator.main.5.weight and shape : torch.Size([256, 128, 4, 4])
model name:  loss.discriminator.main.6.weight and shape : torch.Size([256])
model name:  loss.discriminator.main.6.bias and shape : torch.Size([256])
model name:  loss.discriminator.main.6.running_mean and shape : torch.Size([256])
model name:  loss.discriminator.main.6.running_var and shape : torch.Size([256])
model name:  loss.discriminator.main.6.num_batches_tracked and shape : torch.Size([])
model name:  loss.discriminator.main.8.weight and shape : torch.Size([512, 256, 4, 4])
model name:  loss.discriminator.main.9.weight and shape : torch.Size([512])
model name:  loss.discriminator.main.9.bias and shape : torch.Size([512])
model name:  loss.discriminator.main.9.running_mean and shape : torch.Size([512])
model name:  loss.discriminator.main.9.running_var and shape : torch.Size([512])
model name:  loss.discriminator.main.9.num_batches_tracked and shape : torch.Size([])
model name:  loss.discriminator.main.11.weight and shape : torch.Size([1, 512, 4, 4])
model name:  loss.discriminator.main.11.bias and shape : torch.Size([1])


```